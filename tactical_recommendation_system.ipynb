{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H·ªá Th·ªëng ƒê·ªÅ Xu·∫•t Chi·∫øn Thu·∫≠t B√≥ng ƒê√° S·ª≠ D·ª•ng Random Forest\n",
    "\n",
    "## T√≥m T·∫Øt\n",
    "B√†i b√°o n√†y tr√¨nh b√†y m·ªôt h·ªá th·ªëng machine learning s·ª≠ d·ª•ng Random Forest ƒë·ªÉ ƒë·ªÅ xu·∫•t chi·∫øn thu·∫≠t t·ªëi ∆∞u d·ª±a tr√™n c√°c ch·ªâ s·ªë tr·∫≠n ƒë·∫•u. M·ª•c ti√™u l√† ƒë·∫°t ƒë·ªô ch√≠nh x√°c ‚â•55.56% trong vi·ªác d·ª± ƒëo√°n k·∫øt qu·∫£ tr·∫≠n ƒë·∫•u (Win/Draw/Loss) d·ª±a tr√™n c√°c ch·ªâ s·ªë chi·∫øn thu·∫≠t.\n",
    "\n",
    "**T√°c gi·∫£:** Nghi√™n c·ª©u khoa h·ªçc v·ªÅ ph√¢n t√≠ch b√≥ng ƒë√°  \n",
    "**Ng√†y:** 2026"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Th∆∞ Vi·ªán\n",
    "\n",
    "ƒê·∫ßu ti√™n, ch√∫ng ta import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt cho ph√¢n t√≠ch d·ªØ li·ªáu, tr·ª±c quan h√≥a v√† machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Th∆∞ vi·ªán x·ª≠ l√Ω d·ªØ li·ªáu\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Th∆∞ vi·ªán tr·ª±c quan h√≥a\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Th∆∞ vi·ªán Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    confusion_matrix, \n",
    "    accuracy_score,\n",
    "    roc_curve,\n",
    "    auc\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# C·∫•u h√¨nh hi·ªÉn th·ªã\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 4)\n",
    "\n",
    "print(\"‚úì ƒê√£ import th√†nh c√¥ng t·∫•t c·∫£ th∆∞ vi·ªán!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. T·∫£i v√† Kh√°m Ph√° D·ªØ Li·ªáu\n",
    "\n",
    "Ch√∫ng ta t·∫£i d·ªØ li·ªáu chi·∫øn thu·∫≠t t·ª´ file CSV v√† th·ª±c hi·ªán ph√¢n t√≠ch kh√°m ph√° ban ƒë·∫ßu (EDA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ƒê·ªçc d·ªØ li·ªáu\n",
    "df = pd.read_csv('tactical_dataset_with_results.csv')\n",
    "\n",
    "# Hi·ªÉn th·ªã th√¥ng tin c∆° b·∫£n\n",
    "print(\"=\" * 80)\n",
    "print(\"TH√îNG TIN DATASET\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"S·ªë l∆∞·ª£ng tr·∫≠n ƒë·∫•u: {len(df)}\")\n",
    "print(f\"S·ªë l∆∞·ª£ng ƒë·∫∑c tr∆∞ng: {df.shape[1]}\")\n",
    "print(f\"\\nC√°c c·ªôt trong dataset:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Hi·ªÉn th·ªã m·∫´u d·ªØ li·ªáu\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"5 D√íNG ƒê·∫¶U TI√äN\")\n",
    "print(\"=\" * 80)\n",
    "display(df.head())\n",
    "\n",
    "# Th·ªëng k√™ m√¥ t·∫£\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TH·ªêNG K√ä M√î T·∫¢\")\n",
    "print(\"=\" * 80)\n",
    "display(df.describe())\n",
    "\n",
    "# Ki·ªÉm tra gi√° tr·ªã null\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"KI·ªÇM TRA GI√Å TR·ªä NULL\")\n",
    "print(\"=\" * 80)\n",
    "null_counts = df.isnull().sum()\n",
    "print(null_counts[null_counts > 0] if null_counts.sum() > 0 else \"‚úì Kh√¥ng c√≥ gi√° tr·ªã null!\")\n",
    "\n",
    "# Ph√¢n ph·ªëi k·∫øt qu·∫£\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PH√ÇN PH·ªêI K·∫æT QU·∫¢ TR·∫¨N ƒê·∫§U\")\n",
    "print(\"=\" * 80)\n",
    "result_dist = df['result'].value_counts()\n",
    "result_pct = df['result'].value_counts(normalize=True) * 100\n",
    "result_summary = pd.DataFrame({\n",
    "    'S·ªë l∆∞·ª£ng': result_dist,\n",
    "    'T·ª∑ l·ªá (%)': result_pct\n",
    "})\n",
    "display(result_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tr·ª±c Quan H√≥a D·ªØ Li·ªáu\n",
    "\n",
    "### Bi·ªÉu ƒê·ªì 1: Ph√¢n Ph·ªëi K·∫øt Qu·∫£ Tr·∫≠n ƒê·∫•u\n",
    "\n",
    "Bi·ªÉu ƒë·ªì n√†y hi·ªÉn th·ªã t·ª∑ l·ªá th·∫Øng/h√≤a/thua trong dataset, gi√∫p ch√∫ng ta hi·ªÉu s·ª± c√¢n b·∫±ng c·ªßa c√°c l·ªõp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bi·ªÉu ƒë·ªì c·ªôt\n",
    "result_counts = df['result'].value_counts()\n",
    "colors = ['#2ecc71', '#3498db', '#e74c3c']\n",
    "result_counts.plot(kind='bar', ax=axes[0], color=colors, edgecolor='black', linewidth=1.5)\n",
    "axes[0].set_title('S·ªë L∆∞·ª£ng Tr·∫≠n Theo K·∫øt Qu·∫£', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('K·∫øt Qu·∫£', fontsize=12)\n",
    "axes[0].set_ylabel('S·ªë L∆∞·ª£ng Tr·∫≠n', fontsize=12)\n",
    "axes[0].tick_params(axis='x', rotation=0)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Th√™m gi√° tr·ªã l√™n c·ªôt\n",
    "for i, v in enumerate(result_counts):\n",
    "    axes[0].text(i, v + 50, str(v), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Bi·ªÉu ƒë·ªì tr√≤n\n",
    "result_counts.plot(kind='pie', ax=axes[1], autopct='%1.1f%%', \n",
    "                   colors=colors, startangle=90, explode=(0.05, 0, 0))\n",
    "axes[1].set_title('T·ª∑ L·ªá Ph·∫ßn TrƒÉm K·∫øt Qu·∫£', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('chart_1_result_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Bi·ªÉu ƒë·ªì 1: Ph√¢n ph·ªëi k·∫øt qu·∫£ tr·∫≠n ƒë·∫•u ƒë√£ ƒë∆∞·ª£c t·∫°o!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi·ªÉu ƒê·ªì 2: Ma Tr·∫≠n T∆∞∆°ng Quan C√°c Ch·ªâ S·ªë Chi·∫øn Thu·∫≠t\n",
    "\n",
    "Heatmap n√†y cho th·∫•y m·ªëi t∆∞∆°ng quan gi·ªØa c√°c ch·ªâ s·ªë chi·∫øn thu·∫≠t, gi√∫p x√°c ƒë·ªãnh c√°c ƒë·∫∑c tr∆∞ng quan tr·ªçng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ch·ªçn c√°c ƒë·∫∑c tr∆∞ng s·ªë ƒë·ªÉ t√≠nh correlation\n",
    "tactical_features = [\n",
    "    'possession_ratio', 'num_passes', 'avg_pass_length', 'shots', 'xg',\n",
    "    'pressures', 'tackles', 'interceptions', 'avg_x_position', \n",
    "    'team_width', 'final_third_actions', 'ppda', 'goal_diff'\n",
    "]\n",
    "\n",
    "# T√≠nh ma tr·∫≠n t∆∞∆°ng quan\n",
    "correlation_matrix = df[tactical_features].corr()\n",
    "\n",
    "# V·∫Ω heatmap\n",
    "plt.figure(figsize=(14, 10))\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(correlation_matrix, mask=mask, annot=True, fmt='.2f', \n",
    "            cmap='coolwarm', center=0, square=True, linewidths=1,\n",
    "            cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Ma Tr·∫≠n T∆∞∆°ng Quan C√°c Ch·ªâ S·ªë Chi·∫øn Thu·∫≠t', \n",
    "          fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig('chart_2_correlation_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Bi·ªÉu ƒë·ªì 2: Ma tr·∫≠n t∆∞∆°ng quan ƒë√£ ƒë∆∞·ª£c t·∫°o!\")\n",
    "print(\"\\nC√°c c·∫∑p t∆∞∆°ng quan m·∫°nh nh·∫•t (|r| > 0.7):\")\n",
    "high_corr = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.7:\n",
    "            high_corr.append((\n",
    "                correlation_matrix.columns[i],\n",
    "                correlation_matrix.columns[j],\n",
    "                correlation_matrix.iloc[i, j]\n",
    "            ))\n",
    "for feat1, feat2, corr in sorted(high_corr, key=lambda x: abs(x[2]), reverse=True):\n",
    "    print(f\"  ‚Ä¢ {feat1} ‚Üî {feat2}: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi·ªÉu ƒê·ªì 3: Ph√¢n Ph·ªëi C√°c Ch·ªâ S·ªë Quan Tr·ªçng Theo K·∫øt Qu·∫£\n",
    "\n",
    "Bi·ªÉu ƒë·ªì violin plot hi·ªÉn th·ªã ph√¢n ph·ªëi c·ªßa c√°c ch·ªâ s·ªë chi·∫øn thu·∫≠t quan tr·ªçng nh·∫•t theo t·ª´ng k·∫øt qu·∫£ tr·∫≠n ƒë·∫•u."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ch·ªçn 4 ch·ªâ s·ªë quan tr·ªçng nh·∫•t\n",
    "key_metrics = ['possession_ratio', 'xg', 'goal_diff', 'final_third_actions']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, metric in enumerate(key_metrics):\n",
    "    sns.violinplot(data=df, x='result', y=metric, ax=axes[idx],\n",
    "                   order=['Win', 'Draw', 'Loss'],\n",
    "                   palette=['#2ecc71', '#3498db', '#e74c3c'])\n",
    "    axes[idx].set_title(f'Ph√¢n Ph·ªëi {metric.replace(\"_\", \" \").title()}',\n",
    "                        fontsize=13, fontweight='bold')\n",
    "    axes[idx].set_xlabel('K·∫øt Qu·∫£', fontsize=11)\n",
    "    axes[idx].set_ylabel(metric.replace('_', ' ').title(), fontsize=11)\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.suptitle('Ph√¢n Ph·ªëi Ch·ªâ S·ªë Chi·∫øn Thu·∫≠t Theo K·∫øt Qu·∫£ Tr·∫≠n ƒê·∫•u',\n",
    "             fontsize=16, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.savefig('chart_3_metrics_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Bi·ªÉu ƒë·ªì 3: Ph√¢n ph·ªëi ch·ªâ s·ªë theo k·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c t·∫°o!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Chu·∫©n B·ªã D·ªØ Li·ªáu Cho M√¥ H√¨nh\n",
    "\n",
    "Ch√∫ng ta chu·∫©n b·ªã d·ªØ li·ªáu b·∫±ng c√°ch ch·ªçn c√°c ƒë·∫∑c tr∆∞ng ph√π h·ª£p v√† chia th√†nh t·∫≠p hu·∫•n luy·ªán/ki·ªÉm tra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ch·ªçn c√°c ƒë·∫∑c tr∆∞ng ƒë·∫ßu v√†o (features)\n",
    "feature_columns = [\n",
    "    'possession_ratio', 'num_passes', 'avg_pass_length', 'shots', 'xg',\n",
    "    'pressures', 'tackles', 'interceptions', 'avg_x_position',\n",
    "    'team_width', 'final_third_actions', 'ppda', 'goal_diff'\n",
    "]\n",
    "\n",
    "# Bi·∫øn m·ª•c ti√™u (target)\n",
    "target_column = 'result'\n",
    "\n",
    "# T√°ch X (features) v√† y (target)\n",
    "X = df[feature_columns].copy()\n",
    "y = df[target_column].copy()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CHU·∫®N B·ªä D·ªÆ LI·ªÜU\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"S·ªë l∆∞·ª£ng ƒë·∫∑c tr∆∞ng ƒë·∫ßu v√†o: {len(feature_columns)}\")\n",
    "print(f\"Danh s√°ch ƒë·∫∑c tr∆∞ng:\")\n",
    "for i, feat in enumerate(feature_columns, 1):\n",
    "    print(f\"  {i:2d}. {feat}\")\n",
    "\n",
    "print(f\"\\nBi·∫øn m·ª•c ti√™u: {target_column}\")\n",
    "print(f\"C√°c l·ªõp: {y.unique().tolist()}\")\n",
    "\n",
    "# Chia d·ªØ li·ªáu th√†nh t·∫≠p hu·∫•n luy·ªán v√† ki·ªÉm tra (80-20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"PH√ÇN CHIA D·ªÆ LI·ªÜU\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"T·∫≠p hu·∫•n luy·ªán: {len(X_train)} m·∫´u ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"T·∫≠p ki·ªÉm tra:   {len(X_test)} m·∫´u ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nPh√¢n ph·ªëi k·∫øt qu·∫£ trong t·∫≠p hu·∫•n luy·ªán:\")\n",
    "train_dist = y_train.value_counts().sort_index()\n",
    "for result, count in train_dist.items():\n",
    "    print(f\"  {result}: {count} ({count/len(y_train)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nPh√¢n ph·ªëi k·∫øt qu·∫£ trong t·∫≠p ki·ªÉm tra:\")\n",
    "test_dist = y_test.value_counts().sort_index()\n",
    "for result, count in test_dist.items():\n",
    "    print(f\"  {result}: {count} ({count/len(y_test)*100:.1f}%)\")\n",
    "\n",
    "# Chu·∫©n h√≥a d·ªØ li·ªáu (Feature Scaling)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\n‚úì ƒê√£ chu·∫©n h√≥a d·ªØ li·ªáu v·ªõi StandardScaler!\")\n",
    "print(f\"  Mean: {X_train_scaled.mean():.6f}\")\n",
    "print(f\"  Std:  {X_train_scaled.std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. X√¢y D·ª±ng M√¥ H√¨nh Random Forest\n",
    "\n",
    "### 5.1. M√¥ H√¨nh Baseline\n",
    "\n",
    "Ch√∫ng ta b·∫Øt ƒë·∫ßu v·ªõi m√¥ h√¨nh Random Forest c∆° b·∫£n ƒë·ªÉ ƒë√°nh gi√° hi·ªáu su·∫•t ban ƒë·∫ßu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kh·ªüi t·∫°o m√¥ h√¨nh Random Forest baseline\n",
    "rf_baseline = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Hu·∫•n luy·ªán m√¥ h√¨nh\n",
    "print(\"ƒêang hu·∫•n luy·ªán m√¥ h√¨nh Random Forest baseline...\")\n",
    "rf_baseline.fit(X_train_scaled, y_train)\n",
    "\n",
    "# D·ª± ƒëo√°n\n",
    "y_pred_baseline = rf_baseline.predict(X_test_scaled)\n",
    "y_pred_proba_baseline = rf_baseline.predict_proba(X_test_scaled)\n",
    "\n",
    "# ƒê√°nh gi√°\n",
    "accuracy_baseline = accuracy_score(y_test, y_pred_baseline)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"K·∫æT QU·∫¢ M√î H√åNH BASELINE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"ƒê·ªô ch√≠nh x√°c: {accuracy_baseline*100:.2f}%\")\n",
    "print(f\"M·ª•c ti√™u: ‚â•55.56%\")\n",
    "print(f\"Tr·∫°ng th√°i: {'‚úì ƒê·∫†T' if accuracy_baseline >= 0.5556 else '‚úó CH∆ØA ƒê·∫†T'}\")\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"B√ÅO C√ÅO PH√ÇN LO·∫†I CHI TI·∫æT\")\n",
    "print(\"=\" * 80)\n",
    "print(classification_report(y_test, y_pred_baseline, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. T·ªëi ∆Øu H√≥a Si√™u Tham S·ªë\n",
    "\n",
    "S·ª≠ d·ª•ng GridSearchCV ƒë·ªÉ t√¨m si√™u tham s·ªë t·ªët nh·∫•t cho m√¥ h√¨nh Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ƒê·ªãnh nghƒ©a l∆∞·ªõi tham s·ªë\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"T·ªêI ∆ØU H√ìA SI√äU THAM S·ªê\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"S·ªë l∆∞·ª£ng t·ªï h·ª£p tham s·ªë: {np.prod([len(v) for v in param_grid.values()])}\")\n",
    "print(\"ƒêang th·ª±c hi·ªán GridSearchCV v·ªõi 5-fold cross-validation...\")\n",
    "print(\"(Qu√° tr√¨nh n√†y c√≥ th·ªÉ m·∫•t v√†i ph√∫t)\\n\")\n",
    "\n",
    "# GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"K·∫æT QU·∫¢ T·ªêI ∆ØU H√ìA\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"ƒê·ªô ch√≠nh x√°c t·ªët nh·∫•t (Cross-validation): {grid_search.best_score_*100:.2f}%\")\n",
    "print(f\"\\nSi√™u tham s·ªë t·ªët nh·∫•t:\")\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"  ‚Ä¢ {param}: {value}\")\n",
    "\n",
    "# M√¥ h√¨nh t·ªëi ∆∞u\n",
    "rf_optimized = grid_search.best_estimator_\n",
    "\n",
    "# D·ª± ƒëo√°n v·ªõi m√¥ h√¨nh t·ªëi ∆∞u\n",
    "y_pred_optimized = rf_optimized.predict(X_test_scaled)\n",
    "y_pred_proba_optimized = rf_optimized.predict_proba(X_test_scaled)\n",
    "\n",
    "# ƒê√°nh gi√° m√¥ h√¨nh t·ªëi ∆∞u\n",
    "accuracy_optimized = accuracy_score(y_test, y_pred_optimized)\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"HI·ªÜU SU·∫§T M√î H√åNH T·ªêI ∆ØU\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"ƒê·ªô ch√≠nh x√°c tr√™n t·∫≠p ki·ªÉm tra: {accuracy_optimized*100:.2f}%\")\n",
    "print(f\"M·ª•c ti√™u: ‚â•55.56%\")\n",
    "print(f\"Tr·∫°ng th√°i: {'‚úì ƒê·∫†T' if accuracy_optimized >= 0.5556 else '‚úó CH∆ØA ƒê·∫†T'}\")\n",
    "print(f\"\\nC·∫£i thi·ªán so v·ªõi baseline: {(accuracy_optimized - accuracy_baseline)*100:+.2f}%\")\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"B√ÅO C√ÅO PH√ÇN LO·∫†I CHI TI·∫æT\")\n",
    "print(\"=\" * 80)\n",
    "print(classification_report(y_test, y_pred_optimized, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi·ªÉu ƒê·ªì 4: Ma Tr·∫≠n Nh·∫ßm L·∫´n (Confusion Matrix)\n",
    "\n",
    "Ma tr·∫≠n nh·∫ßm l·∫´n cho th·∫•y m√¥ h√¨nh d·ª± ƒëo√°n ƒë√∫ng/sai ·ªü t·ª´ng l·ªõp, gi√∫p ph√¢n t√≠ch l·ªói chi ti·∫øt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T√≠nh confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_optimized, labels=['Win', 'Draw', 'Loss'])\n",
    "\n",
    "# V·∫Ω confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Win', 'Draw', 'Loss'],\n",
    "            yticklabels=['Win', 'Draw', 'Loss'],\n",
    "            cbar_kws={'label': 'S·ªë l∆∞·ª£ng'},\n",
    "            linewidths=2, linecolor='white')\n",
    "plt.title('Ma Tr·∫≠n Nh·∫ßm L·∫´n - M√¥ H√¨nh Random Forest T·ªëi ∆Øu',\n",
    "          fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xlabel('D·ª± ƒêo√°n', fontsize=13, fontweight='bold')\n",
    "plt.ylabel('Th·ª±c T·∫ø', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Th√™m ph·∫ßn trƒÉm\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        percentage = cm[i, j] / cm[i].sum() * 100\n",
    "        plt.text(j + 0.5, i + 0.7, f'({percentage:.1f}%)',\n",
    "                ha='center', va='center', fontsize=10, color='gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('chart_4_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Bi·ªÉu ƒë·ªì 4: Ma tr·∫≠n nh·∫ßm l·∫´n ƒë√£ ƒë∆∞·ª£c t·∫°o!\")\n",
    "\n",
    "# Ph√¢n t√≠ch chi ti·∫øt\n",
    "print(\"\\nPh√¢n t√≠ch ma tr·∫≠n nh·∫ßm l·∫´n:\")\n",
    "labels = ['Win', 'Draw', 'Loss']\n",
    "for i, label in enumerate(labels):\n",
    "    total = cm[i].sum()\n",
    "    correct = cm[i, i]\n",
    "    accuracy = correct / total * 100 if total > 0 else 0\n",
    "    print(f\"  ‚Ä¢ {label}: {correct}/{total} ƒë√∫ng ({accuracy:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi·ªÉu ƒê·ªì 5: T·∫ßm Quan Tr·ªçng C·ªßa C√°c ƒê·∫∑c Tr∆∞ng\n",
    "\n",
    "Bi·ªÉu ƒë·ªì n√†y x·∫øp h·∫°ng c√°c ch·ªâ s·ªë chi·∫øn thu·∫≠t theo m·ª©c ƒë·ªô quan tr·ªçng trong vi·ªác d·ª± ƒëo√°n k·∫øt qu·∫£."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L·∫•y feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_columns,\n",
    "    'importance': rf_optimized.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# V·∫Ω bi·ªÉu ƒë·ªì\n",
    "plt.figure(figsize=(12, 8))\n",
    "colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(feature_importance)))\n",
    "bars = plt.barh(range(len(feature_importance)), \n",
    "                feature_importance['importance'],\n",
    "                color=colors, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "# T√πy ch·ªânh\n",
    "plt.yticks(range(len(feature_importance)), \n",
    "           [f.replace('_', ' ').title() for f in feature_importance['feature']])\n",
    "plt.xlabel('M·ª©c ƒê·ªô Quan Tr·ªçng', fontsize=13, fontweight='bold')\n",
    "plt.title('T·∫ßm Quan Tr·ªçng C·ªßa C√°c Ch·ªâ S·ªë Chi·∫øn Thu·∫≠t\\n(Random Forest)',\n",
    "          fontsize=16, fontweight='bold', pad=20)\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Th√™m gi√° tr·ªã\n",
    "for i, (idx, row) in enumerate(feature_importance.iterrows()):\n",
    "    plt.text(row['importance'] + 0.002, i, f\"{row['importance']:.4f}\",\n",
    "            va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('chart_5_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Bi·ªÉu ƒë·ªì 5: T·∫ßm quan tr·ªçng c·ªßa ƒë·∫∑c tr∆∞ng ƒë√£ ƒë∆∞·ª£c t·∫°o!\")\n",
    "\n",
    "print(\"\\nTop 5 ch·ªâ s·ªë quan tr·ªçng nh·∫•t:\")\n",
    "for i, (idx, row) in enumerate(feature_importance.head(5).iterrows(), 1):\n",
    "    print(f\"  {i}. {row['feature'].replace('_', ' ').title()}: {row['importance']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi·ªÉu ƒê·ªì 6: ƒê∆∞·ªùng Cong ROC ƒêa L·ªõp\n",
    "\n",
    "ƒê∆∞·ªùng cong ROC cho t·ª´ng l·ªõp (Win/Draw/Loss) v√† ROC trung b√¨nh, ƒë√°nh gi√° kh·∫£ nƒÉng ph√¢n bi·ªát c·ªßa m√¥ h√¨nh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize labels cho ROC multi-class\n",
    "classes = ['Draw', 'Loss', 'Win']  # S·∫Øp x·∫øp theo alphabet ƒë·ªÉ match v·ªõi predict_proba\n",
    "y_test_bin = label_binarize(y_test, classes=classes)\n",
    "n_classes = len(classes)\n",
    "\n",
    "# T√≠nh ROC curve v√† AUC cho t·ª´ng l·ªõp\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "for i, class_name in enumerate(classes):\n",
    "    fpr[i] = dict()\n",
    "    tpr[i] = dict()\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_pred_proba_optimized[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# T√≠nh micro-average ROC curve v√† AUC\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_bin.ravel(), \n",
    "                                           y_pred_proba_optimized.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# V·∫Ω ROC curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = ['#e74c3c', '#3498db', '#2ecc71']\n",
    "class_labels = ['Draw', 'Loss', 'Win']\n",
    "\n",
    "for i, (color, label) in enumerate(zip(colors, class_labels)):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=2.5,\n",
    "            label=f'{label} (AUC = {roc_auc[i]:.3f})')\n",
    "\n",
    "# ƒê∆∞·ªùng micro-average\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"], color='navy', lw=3, linestyle='--',\n",
    "        label=f'Micro-average (AUC = {roc_auc[\"micro\"]:.3f})')\n",
    "\n",
    "# ƒê∆∞·ªùng ng·∫´u nhi√™n\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2, alpha=0.3, label='Ng·∫´u nhi√™n (AUC = 0.500)')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('T·ª∑ L·ªá D∆∞∆°ng T√≠nh Gi·∫£ (False Positive Rate)', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('T·ª∑ L·ªá D∆∞∆°ng T√≠nh Th·∫≠t (True Positive Rate)', fontsize=12, fontweight='bold')\n",
    "plt.title('ƒê∆∞·ªùng Cong ROC ƒêa L·ªõp - M√¥ H√¨nh Random Forest',\n",
    "         fontsize=14, fontweight='bold', pad=20)\n",
    "plt.legend(loc=\"lower right\", fontsize=11, framealpha=0.9)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('chart_6_roc_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Bi·ªÉu ƒë·ªì 6: ƒê∆∞·ªùng cong ROC ƒë√£ ƒë∆∞·ª£c t·∫°o!\")\n",
    "print(\"\\nAUC Score cho t·ª´ng l·ªõp:\")\n",
    "for i, label in enumerate(class_labels):\n",
    "    print(f\"  ‚Ä¢ {label}: {roc_auc[i]:.4f}\")\n",
    "print(f\"  ‚Ä¢ Micro-average: {roc_auc['micro']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi·ªÉu ƒê·ªì 7: So S√°nh Hi·ªáu Su·∫•t M√¥ H√¨nh\n",
    "\n",
    "So s√°nh ƒë·ªô ch√≠nh x√°c gi·ªØa m√¥ h√¨nh baseline v√† m√¥ h√¨nh t·ªëi ∆∞u, c√πng v·ªõi cross-validation scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation cho c·∫£ hai m√¥ h√¨nh\n",
    "print(\"ƒêang th·ª±c hi·ªán 5-fold cross-validation...\\n\")\n",
    "\n",
    "cv_scores_baseline = cross_val_score(rf_baseline, X_train_scaled, y_train, \n",
    "                                     cv=5, scoring='accuracy')\n",
    "cv_scores_optimized = cross_val_score(rf_optimized, X_train_scaled, y_train, \n",
    "                                      cv=5, scoring='accuracy')\n",
    "\n",
    "# Chu·∫©n b·ªã d·ªØ li·ªáu cho bi·ªÉu ƒë·ªì\n",
    "models = ['Baseline', 'T·ªëi ∆Øu']\n",
    "test_scores = [accuracy_baseline * 100, accuracy_optimized * 100]\n",
    "cv_means = [cv_scores_baseline.mean() * 100, cv_scores_optimized.mean() * 100]\n",
    "cv_stds = [cv_scores_baseline.std() * 100, cv_scores_optimized.std() * 100]\n",
    "\n",
    "# V·∫Ω bi·ªÉu ƒë·ªì\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Bi·ªÉu ƒë·ªì 1: So s√°nh ƒë·ªô ch√≠nh x√°c\n",
    "x_pos = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax1.bar(x_pos - width/2, test_scores, width, \n",
    "                label='Test Set', color='#3498db', edgecolor='black', linewidth=1.5)\n",
    "bars2 = ax1.bar(x_pos + width/2, cv_means, width,\n",
    "                label='Cross-Validation (Mean)', color='#2ecc71', \n",
    "                edgecolor='black', linewidth=1.5, yerr=cv_stds, capsize=5)\n",
    "\n",
    "# ƒê∆∞·ªùng m·ª•c ti√™u\n",
    "ax1.axhline(y=55.56, color='red', linestyle='--', linewidth=2.5, \n",
    "           label='M·ª•c ti√™u (55.56%)', alpha=0.7)\n",
    "\n",
    "ax1.set_xlabel('M√¥ H√¨nh', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('ƒê·ªô Ch√≠nh X√°c (%)', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('So S√°nh ƒê·ªô Ch√≠nh X√°c Gi·ªØa C√°c M√¥ H√¨nh',\n",
    "             fontsize=14, fontweight='bold')\n",
    "ax1.set_xticks(x_pos)\n",
    "ax1.set_xticklabels(models)\n",
    "ax1.legend(loc='lower right', fontsize=10)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "ax1.set_ylim([40, 100])\n",
    "\n",
    "# Th√™m gi√° tr·ªã l√™n c·ªôt\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "            f'{height:.2f}%', ha='center', va='bottom', fontweight='bold')\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "            f'{height:.2f}%', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Bi·ªÉu ƒë·ªì 2: Ph√¢n ph·ªëi CV scores\n",
    "cv_data = [\n",
    "    cv_scores_baseline * 100,\n",
    "    cv_scores_optimized * 100\n",
    "]\n",
    "\n",
    "bp = ax2.boxplot(cv_data, labels=models, patch_artist=True,\n",
    "                boxprops=dict(facecolor='#95a5a6', alpha=0.7),\n",
    "                medianprops=dict(color='red', linewidth=2),\n",
    "                whiskerprops=dict(linewidth=1.5),\n",
    "                capprops=dict(linewidth=1.5))\n",
    "\n",
    "# ƒê∆∞·ªùng m·ª•c ti√™u\n",
    "ax2.axhline(y=55.56, color='red', linestyle='--', linewidth=2.5,\n",
    "           label='M·ª•c ti√™u (55.56%)', alpha=0.7)\n",
    "\n",
    "ax2.set_xlabel('M√¥ H√¨nh', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('ƒê·ªô Ch√≠nh X√°c (%)', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Ph√¢n Ph·ªëi Cross-Validation Scores (5-Fold)',\n",
    "             fontsize=14, fontweight='bold')\n",
    "ax2.legend(loc='lower right', fontsize=10)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "ax2.set_ylim([40, 100])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('chart_7_model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Bi·ªÉu ƒë·ªì 7: So s√°nh hi·ªáu su·∫•t m√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c t·∫°o!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"K·∫æT QU·∫¢ CROSS-VALIDATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nBaseline Model:\")\n",
    "print(f\"  Mean: {cv_scores_baseline.mean()*100:.2f}%\")\n",
    "print(f\"  Std:  {cv_scores_baseline.std()*100:.2f}%\")\n",
    "print(f\"  Scores: {[f'{s*100:.2f}%' for s in cv_scores_baseline]}\")\n",
    "\n",
    "print(f\"\\nOptimized Model:\")\n",
    "print(f\"  Mean: {cv_scores_optimized.mean()*100:.2f}%\")\n",
    "print(f\"  Std:  {cv_scores_optimized.std()*100:.2f}%\")\n",
    "print(f\"  Scores: {[f'{s*100:.2f}%' for s in cv_scores_optimized]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. H·ªá Th·ªëng ƒê·ªÅ Xu·∫•t Chi·∫øn Thu·∫≠t\n",
    "\n",
    "Ph·∫ßn n√†y x√¢y d·ª±ng h·ªá th·ªëng ƒë·ªÅ xu·∫•t chi·∫øn thu·∫≠t d·ª±a tr√™n d·ª± ƒëo√°n c·ªßa m√¥ h√¨nh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_tactics(match_stats, model, scaler, feature_names):\n",
    "    \"\"\"\n",
    "    ƒê·ªÅ xu·∫•t chi·∫øn thu·∫≠t d·ª±a tr√™n th·ªëng k√™ tr·∫≠n ƒë·∫•u\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    match_stats : dict\n",
    "        Dictionary ch·ª©a c√°c ch·ªâ s·ªë tr·∫≠n ƒë·∫•u\n",
    "    model : RandomForestClassifier\n",
    "        M√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán\n",
    "    scaler : StandardScaler\n",
    "        Scaler ƒë·ªÉ chu·∫©n h√≥a d·ªØ li·ªáu\n",
    "    feature_names : list\n",
    "        Danh s√°ch t√™n c√°c ƒë·∫∑c tr∆∞ng\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Ch·ª©a d·ª± ƒëo√°n v√† ƒë·ªÅ xu·∫•t chi·∫øn thu·∫≠t\n",
    "    \"\"\"\n",
    "    # Chuy·ªÉn ƒë·ªïi input th√†nh DataFrame\n",
    "    input_df = pd.DataFrame([match_stats], columns=feature_names)\n",
    "    \n",
    "    # Chu·∫©n h√≥a\n",
    "    input_scaled = scaler.transform(input_df)\n",
    "    \n",
    "    # D·ª± ƒëo√°n\n",
    "    prediction = model.predict(input_scaled)[0]\n",
    "    probabilities = model.predict_proba(input_scaled)[0]\n",
    "    \n",
    "    # Mapping probability v·ªõi class\n",
    "    class_probs = dict(zip(model.classes_, probabilities))\n",
    "    \n",
    "    # ƒê·ªÅ xu·∫•t chi·∫øn thu·∫≠t d·ª±a tr√™n feature importance v√† prediction\n",
    "    feature_importance_dict = dict(zip(feature_names, model.feature_importances_))\n",
    "    top_features = sorted(feature_importance_dict.items(), \n",
    "                         key=lambda x: x[1], reverse=True)[:5]\n",
    "    \n",
    "    recommendations = []\n",
    "    \n",
    "    # Ph√¢n t√≠ch v√† ƒë·ªÅ xu·∫•t\n",
    "    if match_stats[0] < 0.45:  # possession_ratio th·∫•p\n",
    "        recommendations.append(\"üîπ TƒÉng c∆∞·ªùng ki·ªÉm so√°t b√≥ng - t·ª∑ l·ªá chi·∫øm b√≥ng hi·ªán t·∫°i th·∫•p\")\n",
    "    \n",
    "    if match_stats[3] < 10:  # shots √≠t\n",
    "        recommendations.append(\"üîπ TƒÉng s·ªë l·∫ßn d·ª©t ƒëi·ªÉm - c·∫ßn t·∫°o nhi·ªÅu c∆° h·ªôi ghi b√†n h∆°n\")\n",
    "    \n",
    "    if match_stats[4] < 1.0:  # xG th·∫•p\n",
    "        recommendations.append(\"üîπ C·∫£i thi·ªán ch·∫•t l∆∞·ª£ng d·ª©t ƒëi·ªÉm - xG (Expected Goals) qu√° th·∫•p\")\n",
    "    \n",
    "    if match_stats[5] < 120:  # pressures th·∫•p\n",
    "        recommendations.append(\"üîπ TƒÉng c∆∞·ªùng pressing - c·∫ßn √©p s√¢n ƒë·ªëi ph∆∞∆°ng nhi·ªÅu h∆°n\")\n",
    "    \n",
    "    if match_stats[10] < 350:  # final_third_actions th·∫•p\n",
    "        recommendations.append(\"üîπ TƒÉng ho·∫°t ƒë·ªông ·ªü 1/3 s√¢n cu·ªëi - c·∫ßn t·∫•n c√¥ng hi·ªáu qu·∫£ h∆°n\")\n",
    "    \n",
    "    if match_stats[12] < 0:  # goal_diff √¢m\n",
    "        recommendations.append(\"‚ö†Ô∏è ƒêang thua - c·∫ßn thay ƒë·ªïi chi·∫øn thu·∫≠t t·∫•n c√¥ng t√≠ch c·ª±c h∆°n\")\n",
    "    \n",
    "    if len(recommendations) == 0:\n",
    "        recommendations.append(\"‚úì Chi·∫øn thu·∫≠t hi·ªán t·∫°i ƒëang hi·ªáu qu·∫£, duy tr√¨ phong ƒë·ªô!\")\n",
    "    \n",
    "    return {\n",
    "        'prediction': prediction,\n",
    "        'probabilities': class_probs,\n",
    "        'recommendations': recommendations,\n",
    "        'top_features': top_features\n",
    "    }\n",
    "\n",
    "# Demo h·ªá th·ªëng ƒë·ªÅ xu·∫•t\n",
    "print(\"=\" * 80)\n",
    "print(\"DEMO H·ªÜ TH·ªêNG ƒê·ªÄ XU·∫§T CHI·∫æN THU·∫¨T\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# V√≠ d·ª• 1: Tr·∫≠n ƒë·∫•u m·∫´u (t·ª´ test set)\n",
    "sample_idx = 0\n",
    "sample_stats = X_test.iloc[sample_idx].values\n",
    "actual_result = y_test.iloc[sample_idx]\n",
    "\n",
    "result = recommend_tactics(sample_stats, rf_optimized, scaler, feature_columns)\n",
    "\n",
    "print(\"\\nTR·∫¨N ƒê·∫§U M·∫™U #1\")\n",
    "print(\"-\" * 80)\n",
    "print(\"Th·ªëng k√™ tr·∫≠n ƒë·∫•u:\")\n",
    "for feat, val in zip(feature_columns, sample_stats):\n",
    "    print(f\"  ‚Ä¢ {feat.replace('_', ' ').title()}: {val:.2f}\")\n",
    "\n",
    "print(f\"\\nK·∫øt qu·∫£ th·ª±c t·∫ø: {actual_result}\")\n",
    "print(f\"D·ª± ƒëo√°n: {result['prediction']}\")\n",
    "print(f\"\\nX√°c su·∫•t d·ª± ƒëo√°n:\")\n",
    "for outcome, prob in sorted(result['probabilities'].items(), \n",
    "                           key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  ‚Ä¢ {outcome}: {prob*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nƒê·ªÅ xu·∫•t chi·∫øn thu·∫≠t:\")\n",
    "for rec in result['recommendations']:\n",
    "    print(f\"  {rec}\")\n",
    "\n",
    "print(f\"\\nTop 5 ch·ªâ s·ªë quan tr·ªçng nh·∫•t:\")\n",
    "for feat, importance in result['top_features']:\n",
    "    print(f\"  ‚Ä¢ {feat.replace('_', ' ').title()}: {importance:.4f}\")\n",
    "\n",
    "# V√≠ d·ª• 2: Tr·∫≠n ƒë·∫•u v·ªõi th·ªëng k√™ t√πy ch·ªânh\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TR·∫¨N ƒê·∫§U M·∫™U #2 (T√πy ch·ªânh)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "custom_stats = [\n",
    "    0.40,  # possession_ratio - th·∫•p\n",
    "    400,   # num_passes\n",
    "    22.0,  # avg_pass_length\n",
    "    8,     # shots - √≠t\n",
    "    0.8,   # xG - th·∫•p\n",
    "    100,   # pressures - th·∫•p\n",
    "    30,    # tackles\n",
    "    10,    # interceptions\n",
    "    50.0,  # avg_x_position\n",
    "    75.0,  # team_width\n",
    "    300,   # final_third_actions - th·∫•p\n",
    "    1.5,   # ppda\n",
    "    -1     # goal_diff - ƒëang thua\n",
    "]\n",
    "\n",
    "result2 = recommend_tactics(custom_stats, rf_optimized, scaler, feature_columns)\n",
    "\n",
    "print(\"D·ª± ƒëo√°n k·∫øt qu·∫£:\")\n",
    "print(f\"  ‚Üí {result2['prediction']} (Confidence: {max(result2['probabilities'].values())*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nƒê·ªÅ xu·∫•t chi·∫øn thu·∫≠t:\")\n",
    "for rec in result2['recommendations']:\n",
    "    print(f\"  {rec}\")\n",
    "\n",
    "print(\"\\n‚úì H·ªá th·ªëng ƒë·ªÅ xu·∫•t chi·∫øn thu·∫≠t ho·∫°t ƒë·ªông t·ªët!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. K·∫øt Lu·∫≠n v√† T√≥m T·∫Øt\n",
    "\n",
    "### 7.1. T·ªïng K·∫øt K·∫øt Qu·∫£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"T√ìM T·∫ÆT K·∫æT QU·∫¢ NGHI√äN C·ª®U\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n1. M·ª§C TI√äU NGHI√äN C·ª®U\")\n",
    "print(\"-\" * 80)\n",
    "print(\"   X√¢y d·ª±ng h·ªá th·ªëng ML ƒë·ªÉ ƒë·ªÅ xu·∫•t chi·∫øn thu·∫≠t b√≥ng ƒë√° d·ª±a tr√™n\")\n",
    "print(\"   c√°c ch·ªâ s·ªë tr·∫≠n ƒë·∫•u v·ªõi ƒë·ªô ch√≠nh x√°c ‚â•55.56%\")\n",
    "\n",
    "print(\"\\n2. D·ªÆ LI·ªÜU\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"   ‚Ä¢ T·ªïng s·ªë m·∫´u: {len(df):,}\")\n",
    "print(f\"   ‚Ä¢ S·ªë ƒë·∫∑c tr∆∞ng: {len(feature_columns)}\")\n",
    "print(f\"   ‚Ä¢ Ph√¢n b·ªë l·ªõp: Win={result_dist['Win']}, Draw={result_dist['Draw']}, Loss={result_dist['Loss']}\")\n",
    "\n",
    "print(\"\\n3. M√î H√åNH\")\n",
    "print(\"-\" * 80)\n",
    "print(\"   ‚Ä¢ Thu·∫≠t to√°n: Random Forest Classifier\")\n",
    "print(\"   ‚Ä¢ T·ªëi ∆∞u h√≥a: GridSearchCV v·ªõi 5-fold cross-validation\")\n",
    "print(\"   ‚Ä¢ Si√™u tham s·ªë t·ªët nh·∫•t:\")\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"     - {param}: {value}\")\n",
    "\n",
    "print(\"\\n4. K·∫æT QU·∫¢ HI·ªÜU SU·∫§T\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"   ‚Ä¢ Baseline Model:\")\n",
    "print(f\"     - Test Accuracy: {accuracy_baseline*100:.2f}%\")\n",
    "print(f\"     - CV Mean: {cv_scores_baseline.mean()*100:.2f}% (¬±{cv_scores_baseline.std()*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\n   ‚Ä¢ Optimized Model:\")\n",
    "print(f\"     - Test Accuracy: {accuracy_optimized*100:.2f}%\")\n",
    "print(f\"     - CV Mean: {cv_scores_optimized.mean()*100:.2f}% (¬±{cv_scores_optimized.std()*100:.2f}%)\")\n",
    "print(f\"     - C·∫£i thi·ªán: +{(accuracy_optimized - accuracy_baseline)*100:.2f}%\")\n",
    "\n",
    "print(f\"\\n   ‚Ä¢ ƒê·∫°t m·ª•c ti√™u: {'‚úì C√ì' if accuracy_optimized >= 0.5556 else '‚úó KH√îNG'}\")\n",
    "print(f\"     ({accuracy_optimized*100:.2f}% {'‚â•' if accuracy_optimized >= 0.5556 else '<'} 55.56%)\")\n",
    "\n",
    "print(\"\\n5. PH√ÅT HI·ªÜN QUAN TR·ªåNG\")\n",
    "print(\"-\" * 80)\n",
    "print(\"   Top 3 ch·ªâ s·ªë quan tr·ªçng nh·∫•t:\")\n",
    "for i, (feat, imp) in enumerate(feature_importance.head(3).values, 1):\n",
    "    print(f\"     {i}. {feat.replace('_', ' ').title()}: {imp:.4f}\")\n",
    "\n",
    "print(\"\\n6. ·ª®NG D·ª§NG TH·ª∞C T·∫æ\")\n",
    "print(\"-\" * 80)\n",
    "print(\"   ‚Ä¢ H·ªá th·ªëng c√≥ th·ªÉ d·ª± ƒëo√°n k·∫øt qu·∫£ tr·∫≠n ƒë·∫•u\")\n",
    "print(\"   ‚Ä¢ ƒê·ªÅ xu·∫•t ƒëi·ªÅu ch·ªânh chi·∫øn thu·∫≠t d·ª±a tr√™n ƒëi·ªÉm y·∫øu\")\n",
    "print(\"   ‚Ä¢ H·ªó tr·ª£ HLV ƒë∆∞a ra quy·∫øt ƒë·ªãnh chi·∫øn thu·∫≠t\")\n",
    "\n",
    "print(\"\\n7. H·∫†NG CH·∫æ V√Ä H∆Ø·ªöNG PH√ÅT TRI·ªÇN\")\n",
    "print(\"-\" * 80)\n",
    "print(\"   H·∫°n ch·∫ø:\")\n",
    "print(\"   ‚Ä¢ Kh√¥ng x√©t ƒë·∫øn y·∫øu t·ªë t√¢m l√Ω, ch·∫•n th∆∞∆°ng\")\n",
    "print(\"   ‚Ä¢ D·ªØ li·ªáu gi·ªõi h·∫°n ·ªü m·ªôt s·ªë gi·∫£i ƒë·∫•u\")\n",
    "print(\"\\n   H∆∞·ªõng ph√°t tri·ªÉn:\")\n",
    "print(\"   ‚Ä¢ Th√™m d·ªØ li·ªáu real-time tracking\")\n",
    "print(\"   ‚Ä¢ K·∫øt h·ª£p deep learning cho video analysis\")\n",
    "print(\"   ‚Ä¢ T√≠ch h·ª£p y·∫øu t·ªë th·ªùi ti·∫øt, s√¢n nh√†/s√¢n kh√°ch\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"K·∫æT LU·∫¨N\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Nghi√™n c·ª©u ƒë√£ th√†nh c√¥ng x√¢y d·ª±ng h·ªá th·ªëng Random Forest ƒë·∫°t\")\n",
    "print(f\"ƒë·ªô ch√≠nh x√°c {accuracy_optimized*100:.2f}%, v∆∞·ª£t m·ª•c ti√™u 55.56%.\")\n",
    "print(\"H·ªá th·ªëng c√≥ th·ªÉ ·ª©ng d·ª•ng th·ª±c t·∫ø ƒë·ªÉ h·ªó tr·ª£ ph√¢n t√≠ch v√† ƒë·ªÅ xu·∫•t\")\n",
    "print(\"chi·∫øn thu·∫≠t trong b√≥ng ƒë√° chuy√™n nghi·ªáp.\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. L∆∞u M√¥ H√¨nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# L∆∞u m√¥ h√¨nh\n",
    "model_filename = 'tactical_rf_model.pkl'\n",
    "scaler_filename = 'tactical_scaler.pkl'\n",
    "\n",
    "with open(model_filename, 'wb') as f:\n",
    "    pickle.dump(rf_optimized, f)\n",
    "\n",
    "with open(scaler_filename, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(\"‚úì ƒê√£ l∆∞u m√¥ h√¨nh v√† scaler!\")\n",
    "print(f\"  ‚Ä¢ Model: {model_filename}\")\n",
    "print(f\"  ‚Ä¢ Scaler: {scaler_filename}\")\n",
    "\n",
    "# H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng\n",
    "print(\"\\nƒê·ªÉ s·ª≠ d·ª•ng m√¥ h√¨nh ƒë√£ l∆∞u:\")\n",
    "print(\"\"\"```python\n",
    "import pickle\n",
    "\n",
    "# Load model\n",
    "with open('tactical_rf_model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "# Load scaler\n",
    "with open('tactical_scaler.pkl', 'rb') as f:\n",
    "    scaler = pickle.load(f)\n",
    "\n",
    "# D·ª± ƒëo√°n\n",
    "new_data = scaler.transform([your_match_stats])\n",
    "prediction = model.predict(new_data)\n",
    "```\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T√†i Li·ªáu Tham Kh·∫£o\n",
    "\n",
    "1. Breiman, L. (2001). Random Forests. Machine Learning, 45(1), 5-32.\n",
    "2. Pedregosa, F., et al. (2011). Scikit-learn: Machine Learning in Python. JMLR 12, pp. 2825-2830.\n",
    "3. Decroos, T., et al. (2019). Actions Speak Louder than Goals: Valuing Player Actions in Soccer. KDD 2019.\n",
    "4. StatsBomb. (2023). Open Data. https://github.com/statsbomb/open-data\n",
    "\n",
    "---\n",
    "\n",
    "**T√°c gi·∫£:** Nghi√™n c·ª©u khoa h·ªçc v·ªÅ ph√¢n t√≠ch b√≥ng ƒë√°  \n",
    "**Ng√†y ho√†n th√†nh:** 2026  \n",
    "**C√¥ng c·ª•:** Python 3.x, scikit-learn, pandas, matplotlib, seaborn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
