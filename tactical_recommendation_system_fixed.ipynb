{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H\u1ec7 Th\u1ed1ng \u0110\u1ec1 Xu\u1ea5t Chi\u1ebfn Thu\u1eadt B\u00f3ng \u0110\u00e1 S\u1eed D\u1ee5ng Random Forest\n",
    "\n",
    "## T\u00f3m T\u1eaft\n",
    "B\u00e0i b\u00e1o n\u00e0y tr\u00ecnh b\u00e0y m\u1ed9t h\u1ec7 th\u1ed1ng machine learning s\u1eed d\u1ee5ng Random Forest \u0111\u1ec3 \u0111\u1ec1 xu\u1ea5t chi\u1ebfn thu\u1eadt t\u1ed1i \u01b0u d\u1ef1a tr\u00ean c\u00e1c ch\u1ec9 s\u1ed1 tr\u1eadn \u0111\u1ea5u. M\u1ee5c ti\u00eau l\u00e0 \u0111\u1ea1t \u0111\u1ed9 ch\u00ednh x\u00e1c \u226555.56% trong vi\u1ec7c d\u1ef1 \u0111o\u00e1n k\u1ebft qu\u1ea3 tr\u1eadn \u0111\u1ea5u (Win/Draw/Loss) d\u1ef1a tr\u00ean c\u00e1c ch\u1ec9 s\u1ed1 chi\u1ebfn thu\u1eadt.\n",
    "\n",
    "**T\u00e1c gi\u1ea3:** Nghi\u00ean c\u1ee9u khoa h\u1ecdc v\u1ec1 ph\u00e2n t\u00edch b\u00f3ng \u0111\u00e1  \n",
    "**Ng\u00e0y:** 2026"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Th\u01b0 Vi\u1ec7n\n",
    "\n",
    "\u0110\u1ea7u ti\u00ean, ch\u00fang ta import c\u00e1c th\u01b0 vi\u1ec7n c\u1ea7n thi\u1ebft cho ph\u00e2n t\u00edch d\u1eef li\u1ec7u, tr\u1ef1c quan h\u00f3a v\u00e0 machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Th\u01b0 vi\u1ec7n x\u1eed l\u00fd d\u1eef li\u1ec7u\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Th\u01b0 vi\u1ec7n tr\u1ef1c quan h\u00f3a\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Th\u01b0 vi\u1ec7n Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    confusion_matrix, \n",
    "    accuracy_score,\n",
    "    roc_curve,\n",
    "    auc\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# C\u1ea5u h\u00ecnh hi\u1ec3n th\u1ecb\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 4)\n",
    "\n",
    "print(\"\u2713 \u0110\u00e3 import th\u00e0nh c\u00f4ng t\u1ea5t c\u1ea3 th\u01b0 vi\u1ec7n!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. T\u1ea3i v\u00e0 Kh\u00e1m Ph\u00e1 D\u1eef Li\u1ec7u\n",
    "\n",
    "Ch\u00fang ta t\u1ea3i d\u1eef li\u1ec7u chi\u1ebfn thu\u1eadt t\u1eeb file CSV v\u00e0 th\u1ef1c hi\u1ec7n ph\u00e2n t\u00edch kh\u00e1m ph\u00e1 ban \u0111\u1ea7u (EDA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u0110\u1ecdc d\u1eef li\u1ec7u\n",
    "df = pd.read_csv('tactical_dataset_with_results.csv')\n",
    "\n",
    "# Hi\u1ec3n th\u1ecb th\u00f4ng tin c\u01a1 b\u1ea3n\n",
    "print(\"=\" * 80)\n",
    "print(\"TH\u00d4NG TIN DATASET\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"S\u1ed1 l\u01b0\u1ee3ng tr\u1eadn \u0111\u1ea5u: {len(df)}\")\n",
    "print(f\"S\u1ed1 l\u01b0\u1ee3ng \u0111\u1eb7c tr\u01b0ng: {df.shape[1]}\")\n",
    "print(f\"\\nC\u00e1c c\u1ed9t trong dataset:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Hi\u1ec3n th\u1ecb m\u1eabu d\u1eef li\u1ec7u\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"5 D\u00d2NG \u0110\u1ea6U TI\u00caN\")\n",
    "print(\"=\" * 80)\n",
    "display(df.head())\n",
    "\n",
    "# Th\u1ed1ng k\u00ea m\u00f4 t\u1ea3\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TH\u1ed0NG K\u00ca M\u00d4 T\u1ea2\")\n",
    "print(\"=\" * 80)\n",
    "display(df.describe())\n",
    "\n",
    "# Ki\u1ec3m tra gi\u00e1 tr\u1ecb null\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"KI\u1ec2M TRA GI\u00c1 TR\u1eca NULL\")\n",
    "print(\"=\" * 80)\n",
    "null_counts = df.isnull().sum()\n",
    "print(null_counts[null_counts > 0] if null_counts.sum() > 0 else \"\u2713 Kh\u00f4ng c\u00f3 gi\u00e1 tr\u1ecb null!\")\n",
    "\n",
    "# Ph\u00e2n ph\u1ed1i k\u1ebft qu\u1ea3\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PH\u00c2N PH\u1ed0I K\u1ebeT QU\u1ea2 TR\u1eacN \u0110\u1ea4U\")\n",
    "print(\"=\" * 80)\n",
    "result_dist = df['result'].value_counts()\n",
    "result_pct = df['result'].value_counts(normalize=True) * 100\n",
    "result_summary = pd.DataFrame({\n",
    "    'S\u1ed1 l\u01b0\u1ee3ng': result_dist,\n",
    "    'T\u1ef7 l\u1ec7 (%)': result_pct\n",
    "})\n",
    "display(result_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tr\u1ef1c Quan H\u00f3a D\u1eef Li\u1ec7u\n",
    "\n",
    "### Bi\u1ec3u \u0110\u1ed3 1: Ph\u00e2n Ph\u1ed1i K\u1ebft Qu\u1ea3 Tr\u1eadn \u0110\u1ea5u\n",
    "\n",
    "Bi\u1ec3u \u0111\u1ed3 n\u00e0y hi\u1ec3n th\u1ecb t\u1ef7 l\u1ec7 th\u1eafng/h\u00f2a/thua trong dataset, gi\u00fap ch\u00fang ta hi\u1ec3u s\u1ef1 c\u00e2n b\u1eb1ng c\u1ee7a c\u00e1c l\u1edbp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bi\u1ec3u \u0111\u1ed3 c\u1ed9t\n",
    "result_counts = df['result'].value_counts()\n",
    "colors = ['#2ecc71', '#3498db', '#e74c3c']\n",
    "result_counts.plot(kind='bar', ax=axes[0], color=colors, edgecolor='black', linewidth=1.5)\n",
    "axes[0].set_title('S\u1ed1 L\u01b0\u1ee3ng Tr\u1eadn Theo K\u1ebft Qu\u1ea3', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('K\u1ebft Qu\u1ea3', fontsize=12)\n",
    "axes[0].set_ylabel('S\u1ed1 L\u01b0\u1ee3ng Tr\u1eadn', fontsize=12)\n",
    "axes[0].tick_params(axis='x', rotation=0)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Th\u00eam gi\u00e1 tr\u1ecb l\u00ean c\u1ed9t\n",
    "for i, v in enumerate(result_counts):\n",
    "    axes[0].text(i, v + 50, str(v), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Bi\u1ec3u \u0111\u1ed3 tr\u00f2n\n",
    "result_counts.plot(kind='pie', ax=axes[1], autopct='%1.1f%%', \n",
    "                   colors=colors, startangle=90, explode=(0.05, 0, 0))\n",
    "axes[1].set_title('T\u1ef7 L\u1ec7 Ph\u1ea7n Tr\u0103m K\u1ebft Qu\u1ea3', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('chart_1_result_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\u2713 Bi\u1ec3u \u0111\u1ed3 1: Ph\u00e2n ph\u1ed1i k\u1ebft qu\u1ea3 tr\u1eadn \u0111\u1ea5u \u0111\u00e3 \u0111\u01b0\u1ee3c t\u1ea1o!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi\u1ec3u \u0110\u1ed3 2: Ma Tr\u1eadn T\u01b0\u01a1ng Quan C\u00e1c Ch\u1ec9 S\u1ed1 Chi\u1ebfn Thu\u1eadt\n",
    "\n",
    "Heatmap n\u00e0y cho th\u1ea5y m\u1ed1i t\u01b0\u01a1ng quan gi\u1eefa c\u00e1c ch\u1ec9 s\u1ed1 chi\u1ebfn thu\u1eadt, gi\u00fap x\u00e1c \u0111\u1ecbnh c\u00e1c \u0111\u1eb7c tr\u01b0ng quan tr\u1ecdng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ch\u1ecdn c\u00e1c \u0111\u1eb7c tr\u01b0ng s\u1ed1 \u0111\u1ec3 t\u00ednh correlation\n",
    "tactical_features = [\n",
    "    'possession_ratio', 'num_passes', 'avg_pass_length', 'shots', 'xg',\n",
    "    'pressures', 'tackles', 'interceptions', 'avg_x_position', \n",
    "    'team_width', 'final_third_actions', 'ppda'\n",
    "]\n",
    "\n",
    "# T\u00ednh ma tr\u1eadn t\u01b0\u01a1ng quan\n",
    "correlation_matrix = df[tactical_features].corr()\n",
    "\n",
    "# V\u1ebd heatmap\n",
    "plt.figure(figsize=(14, 10))\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(correlation_matrix, mask=mask, annot=True, fmt='.2f', \n",
    "            cmap='coolwarm', center=0, square=True, linewidths=1,\n",
    "            cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Ma Tr\u1eadn T\u01b0\u01a1ng Quan C\u00e1c Ch\u1ec9 S\u1ed1 Chi\u1ebfn Thu\u1eadt', \n",
    "          fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig('chart_2_correlation_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\u2713 Bi\u1ec3u \u0111\u1ed3 2: Ma tr\u1eadn t\u01b0\u01a1ng quan \u0111\u00e3 \u0111\u01b0\u1ee3c t\u1ea1o!\")\n",
    "print(\"\\nC\u00e1c c\u1eb7p t\u01b0\u01a1ng quan m\u1ea1nh nh\u1ea5t (|r| > 0.7):\")\n",
    "high_corr = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.7:\n",
    "            high_corr.append((\n",
    "                correlation_matrix.columns[i],\n",
    "                correlation_matrix.columns[j],\n",
    "                correlation_matrix.iloc[i, j]\n",
    "            ))\n",
    "for feat1, feat2, corr in sorted(high_corr, key=lambda x: abs(x[2]), reverse=True):\n",
    "    print(f\"  \u2022 {feat1} \u2194 {feat2}: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi\u1ec3u \u0110\u1ed3 3: Ph\u00e2n Ph\u1ed1i C\u00e1c Ch\u1ec9 S\u1ed1 Quan Tr\u1ecdng Theo K\u1ebft Qu\u1ea3\n",
    "\n",
    "Bi\u1ec3u \u0111\u1ed3 violin plot hi\u1ec3n th\u1ecb ph\u00e2n ph\u1ed1i c\u1ee7a c\u00e1c ch\u1ec9 s\u1ed1 chi\u1ebfn thu\u1eadt quan tr\u1ecdng nh\u1ea5t theo t\u1eebng k\u1ebft qu\u1ea3 tr\u1eadn \u0111\u1ea5u."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ch\u1ecdn 4 ch\u1ec9 s\u1ed1 quan tr\u1ecdng nh\u1ea5t\n",
    "key_metrics = ['possession_ratio', 'xg', 'ppda', 'final_third_actions']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, metric in enumerate(key_metrics):\n",
    "    sns.violinplot(data=df, x='result', y=metric, ax=axes[idx],\n",
    "                   order=['Win', 'Draw', 'Loss'],\n",
    "                   palette=['#2ecc71', '#3498db', '#e74c3c'])\n",
    "    axes[idx].set_title(f'Ph\u00e2n Ph\u1ed1i {metric.replace(\"_\", \" \").title()}',\n",
    "                        fontsize=13, fontweight='bold')\n",
    "    axes[idx].set_xlabel('K\u1ebft Qu\u1ea3', fontsize=11)\n",
    "    axes[idx].set_ylabel(metric.replace('_', ' ').title(), fontsize=11)\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.suptitle('Ph\u00e2n Ph\u1ed1i Ch\u1ec9 S\u1ed1 Chi\u1ebfn Thu\u1eadt Theo K\u1ebft Qu\u1ea3 Tr\u1eadn \u0110\u1ea5u',\n",
    "             fontsize=16, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.savefig('chart_3_metrics_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\u2713 Bi\u1ec3u \u0111\u1ed3 3: Ph\u00e2n ph\u1ed1i ch\u1ec9 s\u1ed1 theo k\u1ebft qu\u1ea3 \u0111\u00e3 \u0111\u01b0\u1ee3c t\u1ea1o!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Chu\u1ea9n B\u1ecb D\u1eef Li\u1ec7u Cho M\u00f4 H\u00ecnh\n",
    "\n",
    "Ch\u00fang ta chu\u1ea9n b\u1ecb d\u1eef li\u1ec7u b\u1eb1ng c\u00e1ch ch\u1ecdn c\u00e1c \u0111\u1eb7c tr\u01b0ng ph\u00f9 h\u1ee3p v\u00e0 chia th\u00e0nh t\u1eadp hu\u1ea5n luy\u1ec7n/ki\u1ec3m tra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ch\u1ecdn c\u00e1c \u0111\u1eb7c tr\u01b0ng \u0111\u1ea7u v\u00e0o (features)\n",
    "feature_columns = [\n",
    "    'possession_ratio', 'num_passes', 'avg_pass_length', 'shots', 'xg',\n",
    "    'pressures', 'tackles', 'interceptions', 'avg_x_position',\n",
    "    'team_width', 'final_third_actions', 'ppda', \n",
    "]\n",
    "\n",
    "# Bi\u1ebfn m\u1ee5c ti\u00eau (target)\n",
    "target_column = 'result'\n",
    "\n",
    "# T\u00e1ch X (features) v\u00e0 y (target)\n",
    "X = df[feature_columns].copy()\n",
    "y = df[target_column].copy()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CHU\u1ea8N B\u1eca D\u1eee LI\u1ec6U\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"S\u1ed1 l\u01b0\u1ee3ng \u0111\u1eb7c tr\u01b0ng \u0111\u1ea7u v\u00e0o: {len(feature_columns)}\")\n",
    "print(f\"Danh s\u00e1ch \u0111\u1eb7c tr\u01b0ng:\")\n",
    "for i, feat in enumerate(feature_columns, 1):\n",
    "    print(f\"  {i:2d}. {feat}\")\n",
    "\n",
    "print(f\"\\nBi\u1ebfn m\u1ee5c ti\u00eau: {target_column}\")\n",
    "print(f\"C\u00e1c l\u1edbp: {y.unique().tolist()}\")\n",
    "\n",
    "# Chia d\u1eef li\u1ec7u th\u00e0nh t\u1eadp hu\u1ea5n luy\u1ec7n v\u00e0 ki\u1ec3m tra (80-20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"PH\u00c2N CHIA D\u1eee LI\u1ec6U\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"T\u1eadp hu\u1ea5n luy\u1ec7n: {len(X_train)} m\u1eabu ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"T\u1eadp ki\u1ec3m tra:   {len(X_test)} m\u1eabu ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nPh\u00e2n ph\u1ed1i k\u1ebft qu\u1ea3 trong t\u1eadp hu\u1ea5n luy\u1ec7n:\")\n",
    "train_dist = y_train.value_counts().sort_index()\n",
    "for result, count in train_dist.items():\n",
    "    print(f\"  {result}: {count} ({count/len(y_train)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nPh\u00e2n ph\u1ed1i k\u1ebft qu\u1ea3 trong t\u1eadp ki\u1ec3m tra:\")\n",
    "test_dist = y_test.value_counts().sort_index()\n",
    "for result, count in test_dist.items():\n",
    "    print(f\"  {result}: {count} ({count/len(y_test)*100:.1f}%)\")\n",
    "\n",
    "# Chu\u1ea9n h\u00f3a d\u1eef li\u1ec7u (Feature Scaling)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\n\u2713 \u0110\u00e3 chu\u1ea9n h\u00f3a d\u1eef li\u1ec7u v\u1edbi StandardScaler!\")\n",
    "print(f\"  Mean: {X_train_scaled.mean():.6f}\")\n",
    "print(f\"  Std:  {X_train_scaled.std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. X\u00e2y D\u1ef1ng M\u00f4 H\u00ecnh Random Forest\n",
    "\n",
    "### 5.1. M\u00f4 H\u00ecnh Baseline\n",
    "\n",
    "Ch\u00fang ta b\u1eaft \u0111\u1ea7u v\u1edbi m\u00f4 h\u00ecnh Random Forest c\u01a1 b\u1ea3n \u0111\u1ec3 \u0111\u00e1nh gi\u00e1 hi\u1ec7u su\u1ea5t ban \u0111\u1ea7u."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kh\u1edfi t\u1ea1o m\u00f4 h\u00ecnh Random Forest baseline\n",
    "rf_baseline = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Hu\u1ea5n luy\u1ec7n m\u00f4 h\u00ecnh\n",
    "print(\"\u0110ang hu\u1ea5n luy\u1ec7n m\u00f4 h\u00ecnh Random Forest baseline...\")\n",
    "rf_baseline.fit(X_train_scaled, y_train)\n",
    "\n",
    "# D\u1ef1 \u0111o\u00e1n\n",
    "y_pred_baseline = rf_baseline.predict(X_test_scaled)\n",
    "y_pred_proba_baseline = rf_baseline.predict_proba(X_test_scaled)\n",
    "\n",
    "# \u0110\u00e1nh gi\u00e1\n",
    "accuracy_baseline = accuracy_score(y_test, y_pred_baseline)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"K\u1ebeT QU\u1ea2 M\u00d4 H\u00ccNH BASELINE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\u0110\u1ed9 ch\u00ednh x\u00e1c: {accuracy_baseline*100:.2f}%\")\n",
    "print(f\"M\u1ee5c ti\u00eau: \u226555.56%\")\n",
    "print(f\"Tr\u1ea1ng th\u00e1i: {'\u2713 \u0110\u1ea0T' if accuracy_baseline >= 0.5556 else '\u2717 CH\u01afA \u0110\u1ea0T'}\")\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"B\u00c1O C\u00c1O PH\u00c2N LO\u1ea0I CHI TI\u1ebeT\")\n",
    "print(\"=\" * 80)\n",
    "print(classification_report(y_test, y_pred_baseline, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. T\u1ed1i \u01afu H\u00f3a Si\u00eau Tham S\u1ed1\n",
    "\n",
    "S\u1eed d\u1ee5ng GridSearchCV \u0111\u1ec3 t\u00ecm si\u00eau tham s\u1ed1 t\u1ed1t nh\u1ea5t cho m\u00f4 h\u00ecnh Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u0110\u1ecbnh ngh\u0129a l\u01b0\u1edbi tham s\u1ed1\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"T\u1ed0I \u01afU H\u00d3A SI\u00caU THAM S\u1ed0\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"S\u1ed1 l\u01b0\u1ee3ng t\u1ed5 h\u1ee3p tham s\u1ed1: {np.prod([len(v) for v in param_grid.values()])}\")\n",
    "print(\"\u0110ang th\u1ef1c hi\u1ec7n GridSearchCV v\u1edbi 5-fold cross-validation...\")\n",
    "print(\"(Qu\u00e1 tr\u00ecnh n\u00e0y c\u00f3 th\u1ec3 m\u1ea5t v\u00e0i ph\u00fat)\\n\")\n",
    "\n",
    "# GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"K\u1ebeT QU\u1ea2 T\u1ed0I \u01afU H\u00d3A\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\u0110\u1ed9 ch\u00ednh x\u00e1c t\u1ed1t nh\u1ea5t (Cross-validation): {grid_search.best_score_*100:.2f}%\")\n",
    "print(f\"\\nSi\u00eau tham s\u1ed1 t\u1ed1t nh\u1ea5t:\")\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"  \u2022 {param}: {value}\")\n",
    "\n",
    "# M\u00f4 h\u00ecnh t\u1ed1i \u01b0u\n",
    "rf_optimized = grid_search.best_estimator_\n",
    "\n",
    "# D\u1ef1 \u0111o\u00e1n v\u1edbi m\u00f4 h\u00ecnh t\u1ed1i \u01b0u\n",
    "y_pred_optimized = rf_optimized.predict(X_test_scaled)\n",
    "y_pred_proba_optimized = rf_optimized.predict_proba(X_test_scaled)\n",
    "\n",
    "# \u0110\u00e1nh gi\u00e1 m\u00f4 h\u00ecnh t\u1ed1i \u01b0u\n",
    "accuracy_optimized = accuracy_score(y_test, y_pred_optimized)\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"HI\u1ec6U SU\u1ea4T M\u00d4 H\u00ccNH T\u1ed0I \u01afU\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\u0110\u1ed9 ch\u00ednh x\u00e1c tr\u00ean t\u1eadp ki\u1ec3m tra: {accuracy_optimized*100:.2f}%\")\n",
    "print(f\"M\u1ee5c ti\u00eau: \u226555.56%\")\n",
    "print(f\"Tr\u1ea1ng th\u00e1i: {'\u2713 \u0110\u1ea0T' if accuracy_optimized >= 0.5556 else '\u2717 CH\u01afA \u0110\u1ea0T'}\")\n",
    "print(f\"\\nC\u1ea3i thi\u1ec7n so v\u1edbi baseline: {(accuracy_optimized - accuracy_baseline)*100:+.2f}%\")\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"B\u00c1O C\u00c1O PH\u00c2N LO\u1ea0I CHI TI\u1ebeT\")\n",
    "print(\"=\" * 80)\n",
    "print(classification_report(y_test, y_pred_optimized, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi\u1ec3u \u0110\u1ed3 4: Ma Tr\u1eadn Nh\u1ea7m L\u1eabn (Confusion Matrix)\n",
    "\n",
    "Ma tr\u1eadn nh\u1ea7m l\u1eabn cho th\u1ea5y m\u00f4 h\u00ecnh d\u1ef1 \u0111o\u00e1n \u0111\u00fang/sai \u1edf t\u1eebng l\u1edbp, gi\u00fap ph\u00e2n t\u00edch l\u1ed7i chi ti\u1ebft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T\u00ednh confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_optimized, labels=['Win', 'Draw', 'Loss'])\n",
    "\n",
    "# V\u1ebd confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Win', 'Draw', 'Loss'],\n",
    "            yticklabels=['Win', 'Draw', 'Loss'],\n",
    "            cbar_kws={'label': 'S\u1ed1 l\u01b0\u1ee3ng'},\n",
    "            linewidths=2, linecolor='white')\n",
    "plt.title('Ma Tr\u1eadn Nh\u1ea7m L\u1eabn - M\u00f4 H\u00ecnh Random Forest T\u1ed1i \u01afu',\n",
    "          fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xlabel('D\u1ef1 \u0110o\u00e1n', fontsize=13, fontweight='bold')\n",
    "plt.ylabel('Th\u1ef1c T\u1ebf', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Th\u00eam ph\u1ea7n tr\u0103m\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        percentage = cm[i, j] / cm[i].sum() * 100\n",
    "        plt.text(j + 0.5, i + 0.7, f'({percentage:.1f}%)',\n",
    "                ha='center', va='center', fontsize=10, color='gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('chart_4_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\u2713 Bi\u1ec3u \u0111\u1ed3 4: Ma tr\u1eadn nh\u1ea7m l\u1eabn \u0111\u00e3 \u0111\u01b0\u1ee3c t\u1ea1o!\")\n",
    "\n",
    "# Ph\u00e2n t\u00edch chi ti\u1ebft\n",
    "print(\"\\nPh\u00e2n t\u00edch ma tr\u1eadn nh\u1ea7m l\u1eabn:\")\n",
    "labels = ['Win', 'Draw', 'Loss']\n",
    "for i, label in enumerate(labels):\n",
    "    total = cm[i].sum()\n",
    "    correct = cm[i, i]\n",
    "    accuracy = correct / total * 100 if total > 0 else 0\n",
    "    print(f\"  \u2022 {label}: {correct}/{total} \u0111\u00fang ({accuracy:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi\u1ec3u \u0110\u1ed3 5: T\u1ea7m Quan Tr\u1ecdng C\u1ee7a C\u00e1c \u0110\u1eb7c Tr\u01b0ng\n",
    "\n",
    "Bi\u1ec3u \u0111\u1ed3 n\u00e0y x\u1ebfp h\u1ea1ng c\u00e1c ch\u1ec9 s\u1ed1 chi\u1ebfn thu\u1eadt theo m\u1ee9c \u0111\u1ed9 quan tr\u1ecdng trong vi\u1ec7c d\u1ef1 \u0111o\u00e1n k\u1ebft qu\u1ea3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L\u1ea5y feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_columns,\n",
    "    'importance': rf_optimized.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# V\u1ebd bi\u1ec3u \u0111\u1ed3\n",
    "plt.figure(figsize=(12, 8))\n",
    "colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(feature_importance)))\n",
    "bars = plt.barh(range(len(feature_importance)), \n",
    "                feature_importance['importance'],\n",
    "                color=colors, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "# T\u00f9y ch\u1ec9nh\n",
    "plt.yticks(range(len(feature_importance)), \n",
    "           [f.replace('_', ' ').title() for f in feature_importance['feature']])\n",
    "plt.xlabel('M\u1ee9c \u0110\u1ed9 Quan Tr\u1ecdng', fontsize=13, fontweight='bold')\n",
    "plt.title('T\u1ea7m Quan Tr\u1ecdng C\u1ee7a C\u00e1c Ch\u1ec9 S\u1ed1 Chi\u1ebfn Thu\u1eadt\\n(Random Forest)',\n",
    "          fontsize=16, fontweight='bold', pad=20)\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Th\u00eam gi\u00e1 tr\u1ecb\n",
    "for i, (idx, row) in enumerate(feature_importance.iterrows()):\n",
    "    plt.text(row['importance'] + 0.002, i, f\"{row['importance']:.4f}\",\n",
    "            va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('chart_5_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\u2713 Bi\u1ec3u \u0111\u1ed3 5: T\u1ea7m quan tr\u1ecdng c\u1ee7a \u0111\u1eb7c tr\u01b0ng \u0111\u00e3 \u0111\u01b0\u1ee3c t\u1ea1o!\")\n",
    "\n",
    "print(\"\\nTop 5 ch\u1ec9 s\u1ed1 quan tr\u1ecdng nh\u1ea5t:\")\n",
    "for i, (idx, row) in enumerate(feature_importance.head(5).iterrows(), 1):\n",
    "    print(f\"  {i}. {row['feature'].replace('_', ' ').title()}: {row['importance']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi\u1ec3u \u0110\u1ed3 6: \u0110\u01b0\u1eddng Cong ROC \u0110a L\u1edbp\n",
    "\n",
    "\u0110\u01b0\u1eddng cong ROC cho t\u1eebng l\u1edbp (Win/Draw/Loss) v\u00e0 ROC trung b\u00ecnh, \u0111\u00e1nh gi\u00e1 kh\u1ea3 n\u0103ng ph\u00e2n bi\u1ec7t c\u1ee7a m\u00f4 h\u00ecnh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize labels cho ROC multi-class\n",
    "classes = ['Draw', 'Loss', 'Win']  # S\u1eafp x\u1ebfp theo alphabet \u0111\u1ec3 match v\u1edbi predict_proba\n",
    "y_test_bin = label_binarize(y_test, classes=classes)\n",
    "n_classes = len(classes)\n",
    "\n",
    "# T\u00ednh ROC curve v\u00e0 AUC cho t\u1eebng l\u1edbp\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "for i, class_name in enumerate(classes):\n",
    "    fpr[i] = dict()\n",
    "    tpr[i] = dict()\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_pred_proba_optimized[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# T\u00ednh micro-average ROC curve v\u00e0 AUC\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_bin.ravel(), \n",
    "                                           y_pred_proba_optimized.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# V\u1ebd ROC curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = ['#e74c3c', '#3498db', '#2ecc71']\n",
    "class_labels = ['Draw', 'Loss', 'Win']\n",
    "\n",
    "for i, (color, label) in enumerate(zip(colors, class_labels)):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=2.5,\n",
    "            label=f'{label} (AUC = {roc_auc[i]:.3f})')\n",
    "\n",
    "# \u0110\u01b0\u1eddng micro-average\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"], color='navy', lw=3, linestyle='--',\n",
    "        label=f'Micro-average (AUC = {roc_auc[\"micro\"]:.3f})')\n",
    "\n",
    "# \u0110\u01b0\u1eddng ng\u1eabu nhi\u00ean\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2, alpha=0.3, label='Ng\u1eabu nhi\u00ean (AUC = 0.500)')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('T\u1ef7 L\u1ec7 D\u01b0\u01a1ng T\u00ednh Gi\u1ea3 (False Positive Rate)', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('T\u1ef7 L\u1ec7 D\u01b0\u01a1ng T\u00ednh Th\u1eadt (True Positive Rate)', fontsize=12, fontweight='bold')\n",
    "plt.title('\u0110\u01b0\u1eddng Cong ROC \u0110a L\u1edbp - M\u00f4 H\u00ecnh Random Forest',\n",
    "         fontsize=14, fontweight='bold', pad=20)\n",
    "plt.legend(loc=\"lower right\", fontsize=11, framealpha=0.9)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('chart_6_roc_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\u2713 Bi\u1ec3u \u0111\u1ed3 6: \u0110\u01b0\u1eddng cong ROC \u0111\u00e3 \u0111\u01b0\u1ee3c t\u1ea1o!\")\n",
    "print(\"\\nAUC Score cho t\u1eebng l\u1edbp:\")\n",
    "for i, label in enumerate(class_labels):\n",
    "    print(f\"  \u2022 {label}: {roc_auc[i]:.4f}\")\n",
    "print(f\"  \u2022 Micro-average: {roc_auc['micro']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi\u1ec3u \u0110\u1ed3 7: So S\u00e1nh Hi\u1ec7u Su\u1ea5t M\u00f4 H\u00ecnh\n",
    "\n",
    "So s\u00e1nh \u0111\u1ed9 ch\u00ednh x\u00e1c gi\u1eefa m\u00f4 h\u00ecnh baseline v\u00e0 m\u00f4 h\u00ecnh t\u1ed1i \u01b0u, c\u00f9ng v\u1edbi cross-validation scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation cho c\u1ea3 hai m\u00f4 h\u00ecnh\n",
    "print(\"\u0110ang th\u1ef1c hi\u1ec7n 5-fold cross-validation...\\n\")\n",
    "\n",
    "cv_scores_baseline = cross_val_score(rf_baseline, X_train_scaled, y_train, \n",
    "                                     cv=5, scoring='accuracy')\n",
    "cv_scores_optimized = cross_val_score(rf_optimized, X_train_scaled, y_train, \n",
    "                                      cv=5, scoring='accuracy')\n",
    "\n",
    "# Chu\u1ea9n b\u1ecb d\u1eef li\u1ec7u cho bi\u1ec3u \u0111\u1ed3\n",
    "models = ['Baseline', 'T\u1ed1i \u01afu']\n",
    "test_scores = [accuracy_baseline * 100, accuracy_optimized * 100]\n",
    "cv_means = [cv_scores_baseline.mean() * 100, cv_scores_optimized.mean() * 100]\n",
    "cv_stds = [cv_scores_baseline.std() * 100, cv_scores_optimized.std() * 100]\n",
    "\n",
    "# V\u1ebd bi\u1ec3u \u0111\u1ed3\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Bi\u1ec3u \u0111\u1ed3 1: So s\u00e1nh \u0111\u1ed9 ch\u00ednh x\u00e1c\n",
    "x_pos = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax1.bar(x_pos - width/2, test_scores, width, \n",
    "                label='Test Set', color='#3498db', edgecolor='black', linewidth=1.5)\n",
    "bars2 = ax1.bar(x_pos + width/2, cv_means, width,\n",
    "                label='Cross-Validation (Mean)', color='#2ecc71', \n",
    "                edgecolor='black', linewidth=1.5, yerr=cv_stds, capsize=5)\n",
    "\n",
    "# \u0110\u01b0\u1eddng m\u1ee5c ti\u00eau\n",
    "ax1.axhline(y=55.56, color='red', linestyle='--', linewidth=2.5, \n",
    "           label='M\u1ee5c ti\u00eau (55.56%)', alpha=0.7)\n",
    "\n",
    "ax1.set_xlabel('M\u00f4 H\u00ecnh', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('\u0110\u1ed9 Ch\u00ednh X\u00e1c (%)', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('So S\u00e1nh \u0110\u1ed9 Ch\u00ednh X\u00e1c Gi\u1eefa C\u00e1c M\u00f4 H\u00ecnh',\n",
    "             fontsize=14, fontweight='bold')\n",
    "ax1.set_xticks(x_pos)\n",
    "ax1.set_xticklabels(models)\n",
    "ax1.legend(loc='lower right', fontsize=10)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "ax1.set_ylim([40, 100])\n",
    "\n",
    "# Th\u00eam gi\u00e1 tr\u1ecb l\u00ean c\u1ed9t\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "            f'{height:.2f}%', ha='center', va='bottom', fontweight='bold')\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "            f'{height:.2f}%', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Bi\u1ec3u \u0111\u1ed3 2: Ph\u00e2n ph\u1ed1i CV scores\n",
    "cv_data = [\n",
    "    cv_scores_baseline * 100,\n",
    "    cv_scores_optimized * 100\n",
    "]\n",
    "\n",
    "bp = ax2.boxplot(cv_data, labels=models, patch_artist=True,\n",
    "                boxprops=dict(facecolor='#95a5a6', alpha=0.7),\n",
    "                medianprops=dict(color='red', linewidth=2),\n",
    "                whiskerprops=dict(linewidth=1.5),\n",
    "                capprops=dict(linewidth=1.5))\n",
    "\n",
    "# \u0110\u01b0\u1eddng m\u1ee5c ti\u00eau\n",
    "ax2.axhline(y=55.56, color='red', linestyle='--', linewidth=2.5,\n",
    "           label='M\u1ee5c ti\u00eau (55.56%)', alpha=0.7)\n",
    "\n",
    "ax2.set_xlabel('M\u00f4 H\u00ecnh', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('\u0110\u1ed9 Ch\u00ednh X\u00e1c (%)', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Ph\u00e2n Ph\u1ed1i Cross-Validation Scores (5-Fold)',\n",
    "             fontsize=14, fontweight='bold')\n",
    "ax2.legend(loc='lower right', fontsize=10)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "ax2.set_ylim([40, 100])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('chart_7_model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\u2713 Bi\u1ec3u \u0111\u1ed3 7: So s\u00e1nh hi\u1ec7u su\u1ea5t m\u00f4 h\u00ecnh \u0111\u00e3 \u0111\u01b0\u1ee3c t\u1ea1o!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"K\u1ebeT QU\u1ea2 CROSS-VALIDATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nBaseline Model:\")\n",
    "print(f\"  Mean: {cv_scores_baseline.mean()*100:.2f}%\")\n",
    "print(f\"  Std:  {cv_scores_baseline.std()*100:.2f}%\")\n",
    "print(f\"  Scores: {[f'{s*100:.2f}%' for s in cv_scores_baseline]}\")\n",
    "\n",
    "print(f\"\\nOptimized Model:\")\n",
    "print(f\"  Mean: {cv_scores_optimized.mean()*100:.2f}%\")\n",
    "print(f\"  Std:  {cv_scores_optimized.std()*100:.2f}%\")\n",
    "print(f\"  Scores: {[f'{s*100:.2f}%' for s in cv_scores_optimized]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. H\u1ec7 Th\u1ed1ng \u0110\u1ec1 Xu\u1ea5t Chi\u1ebfn Thu\u1eadt\n",
    "\n",
    "Ph\u1ea7n n\u00e0y x\u00e2y d\u1ef1ng h\u1ec7 th\u1ed1ng \u0111\u1ec1 xu\u1ea5t chi\u1ebfn thu\u1eadt d\u1ef1a tr\u00ean d\u1ef1 \u0111o\u00e1n c\u1ee7a m\u00f4 h\u00ecnh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_tactics(match_stats, model, scaler, feature_names):\n",
    "    \"\"\"\n",
    "    \u0110\u1ec1 xu\u1ea5t chi\u1ebfn thu\u1eadt d\u1ef1a tr\u00ean th\u1ed1ng k\u00ea tr\u1eadn \u0111\u1ea5u\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    match_stats : dict\n",
    "        Dictionary ch\u1ee9a c\u00e1c ch\u1ec9 s\u1ed1 tr\u1eadn \u0111\u1ea5u\n",
    "    model : RandomForestClassifier\n",
    "        M\u00f4 h\u00ecnh \u0111\u00e3 \u0111\u01b0\u1ee3c hu\u1ea5n luy\u1ec7n\n",
    "    scaler : StandardScaler\n",
    "        Scaler \u0111\u1ec3 chu\u1ea9n h\u00f3a d\u1eef li\u1ec7u\n",
    "    feature_names : list\n",
    "        Danh s\u00e1ch t\u00ean c\u00e1c \u0111\u1eb7c tr\u01b0ng\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Ch\u1ee9a d\u1ef1 \u0111o\u00e1n v\u00e0 \u0111\u1ec1 xu\u1ea5t chi\u1ebfn thu\u1eadt\n",
    "    \"\"\"\n",
    "    # Chuy\u1ec3n \u0111\u1ed5i input th\u00e0nh DataFrame\n",
    "    input_df = pd.DataFrame([match_stats], columns=feature_names)\n",
    "    \n",
    "    # Chu\u1ea9n h\u00f3a\n",
    "    input_scaled = scaler.transform(input_df)\n",
    "    \n",
    "    # D\u1ef1 \u0111o\u00e1n\n",
    "    prediction = model.predict(input_scaled)[0]\n",
    "    probabilities = model.predict_proba(input_scaled)[0]\n",
    "    \n",
    "    # Mapping probability v\u1edbi class\n",
    "    class_probs = dict(zip(model.classes_, probabilities))\n",
    "    \n",
    "    # \u0110\u1ec1 xu\u1ea5t chi\u1ebfn thu\u1eadt d\u1ef1a tr\u00ean feature importance v\u00e0 prediction\n",
    "    feature_importance_dict = dict(zip(feature_names, model.feature_importances_))\n",
    "    top_features = sorted(feature_importance_dict.items(), \n",
    "                         key=lambda x: x[1], reverse=True)[:5]\n",
    "    \n",
    "    recommendations = []\n",
    "    \n",
    "    # Ph\u00e2n t\u00edch v\u00e0 \u0111\u1ec1 xu\u1ea5t\n",
    "    if match_stats[0] < 0.45:  # possession_ratio th\u1ea5p\n",
    "        recommendations.append(\"\ud83d\udd39 T\u0103ng c\u01b0\u1eddng ki\u1ec3m so\u00e1t b\u00f3ng - t\u1ef7 l\u1ec7 chi\u1ebfm b\u00f3ng hi\u1ec7n t\u1ea1i th\u1ea5p\")\n",
    "    \n",
    "    if match_stats[3] < 10:  # shots \u00edt\n",
    "        recommendations.append(\"\ud83d\udd39 T\u0103ng s\u1ed1 l\u1ea7n d\u1ee9t \u0111i\u1ec3m - c\u1ea7n t\u1ea1o nhi\u1ec1u c\u01a1 h\u1ed9i ghi b\u00e0n h\u01a1n\")\n",
    "    \n",
    "    if match_stats[4] < 1.0:  # xG th\u1ea5p\n",
    "        recommendations.append(\"\ud83d\udd39 C\u1ea3i thi\u1ec7n ch\u1ea5t l\u01b0\u1ee3ng d\u1ee9t \u0111i\u1ec3m - xG (Expected Goals) qu\u00e1 th\u1ea5p\")\n",
    "    \n",
    "    if match_stats[5] < 120:  # pressures th\u1ea5p\n",
    "        recommendations.append(\"\ud83d\udd39 T\u0103ng c\u01b0\u1eddng pressing - c\u1ea7n \u00e9p s\u00e2n \u0111\u1ed1i ph\u01b0\u01a1ng nhi\u1ec1u h\u01a1n\")\n",
    "    \n",
    "    if match_stats[10] < 350:  # final_third_actions th\u1ea5p\n",
    "        recommendations.append(\"\ud83d\udd39 T\u0103ng ho\u1ea1t \u0111\u1ed9ng \u1edf 1/3 s\u00e2n cu\u1ed1i - c\u1ea7n t\u1ea5n c\u00f4ng hi\u1ec7u qu\u1ea3 h\u01a1n\")\n",
    "    \n",
    "    if match_stats[12] < 0:  # goal_diff \u00e2m\n",
    "        recommendations.append(\"\u26a0\ufe0f \u0110ang thua - c\u1ea7n thay \u0111\u1ed5i chi\u1ebfn thu\u1eadt t\u1ea5n c\u00f4ng t\u00edch c\u1ef1c h\u01a1n\")\n",
    "    \n",
    "    if len(recommendations) == 0:\n",
    "        recommendations.append(\"\u2713 Chi\u1ebfn thu\u1eadt hi\u1ec7n t\u1ea1i \u0111ang hi\u1ec7u qu\u1ea3, duy tr\u00ec phong \u0111\u1ed9!\")\n",
    "    \n",
    "    return {\n",
    "        'prediction': prediction,\n",
    "        'probabilities': class_probs,\n",
    "        'recommendations': recommendations,\n",
    "        'top_features': top_features\n",
    "    }\n",
    "\n",
    "# Demo h\u1ec7 th\u1ed1ng \u0111\u1ec1 xu\u1ea5t\n",
    "print(\"=\" * 80)\n",
    "print(\"DEMO H\u1ec6 TH\u1ed0NG \u0110\u1ec0 XU\u1ea4T CHI\u1ebeN THU\u1eacT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# V\u00ed d\u1ee5 1: Tr\u1eadn \u0111\u1ea5u m\u1eabu (t\u1eeb test set)\n",
    "sample_idx = 0\n",
    "sample_stats = X_test.iloc[sample_idx].values\n",
    "actual_result = y_test.iloc[sample_idx]\n",
    "\n",
    "result = recommend_tactics(sample_stats, rf_optimized, scaler, feature_columns)\n",
    "\n",
    "print(\"\\nTR\u1eacN \u0110\u1ea4U M\u1eaaU #1\")\n",
    "print(\"-\" * 80)\n",
    "print(\"Th\u1ed1ng k\u00ea tr\u1eadn \u0111\u1ea5u:\")\n",
    "for feat, val in zip(feature_columns, sample_stats):\n",
    "    print(f\"  \u2022 {feat.replace('_', ' ').title()}: {val:.2f}\")\n",
    "\n",
    "print(f\"\\nK\u1ebft qu\u1ea3 th\u1ef1c t\u1ebf: {actual_result}\")\n",
    "print(f\"D\u1ef1 \u0111o\u00e1n: {result['prediction']}\")\n",
    "print(f\"\\nX\u00e1c su\u1ea5t d\u1ef1 \u0111o\u00e1n:\")\n",
    "for outcome, prob in sorted(result['probabilities'].items(), \n",
    "                           key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  \u2022 {outcome}: {prob*100:.2f}%\")\n",
    "\n",
    "print(f\"\\n\u0110\u1ec1 xu\u1ea5t chi\u1ebfn thu\u1eadt:\")\n",
    "for rec in result['recommendations']:\n",
    "    print(f\"  {rec}\")\n",
    "\n",
    "print(f\"\\nTop 5 ch\u1ec9 s\u1ed1 quan tr\u1ecdng nh\u1ea5t:\")\n",
    "for feat, importance in result['top_features']:\n",
    "    print(f\"  \u2022 {feat.replace('_', ' ').title()}: {importance:.4f}\")\n",
    "\n",
    "# V\u00ed d\u1ee5 2: Tr\u1eadn \u0111\u1ea5u v\u1edbi th\u1ed1ng k\u00ea t\u00f9y ch\u1ec9nh\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TR\u1eacN \u0110\u1ea4U M\u1eaaU #2 (T\u00f9y ch\u1ec9nh)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "custom_stats = [\n",
    "    0.40,  # possession_ratio - th\u1ea5p\n",
    "    400,   # num_passes\n",
    "    22.0,  # avg_pass_length\n",
    "    8,     # shots - \u00edt\n",
    "    0.8,   # xG - th\u1ea5p\n",
    "    100,   # pressures - th\u1ea5p\n",
    "    30,    # tackles\n",
    "    10,    # interceptions\n",
    "    50.0,  # avg_x_position\n",
    "    75.0,  # team_width\n",
    "    300,   # final_third_actions - th\u1ea5p\n",
    "    1.5,   # ppda\n",
    "    -1     # goal_diff - \u0111ang thua\n",
    "]\n",
    "\n",
    "result2 = recommend_tactics(custom_stats, rf_optimized, scaler, feature_columns)\n",
    "\n",
    "print(\"D\u1ef1 \u0111o\u00e1n k\u1ebft qu\u1ea3:\")\n",
    "print(f\"  \u2192 {result2['prediction']} (Confidence: {max(result2['probabilities'].values())*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n\u0110\u1ec1 xu\u1ea5t chi\u1ebfn thu\u1eadt:\")\n",
    "for rec in result2['recommendations']:\n",
    "    print(f\"  {rec}\")\n",
    "\n",
    "print(\"\\n\u2713 H\u1ec7 th\u1ed1ng \u0111\u1ec1 xu\u1ea5t chi\u1ebfn thu\u1eadt ho\u1ea1t \u0111\u1ed9ng t\u1ed1t!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. K\u1ebft Lu\u1eadn v\u00e0 T\u00f3m T\u1eaft\n",
    "\n",
    "### 7.1. T\u1ed5ng K\u1ebft K\u1ebft Qu\u1ea3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"T\u00d3M T\u1eaeT K\u1ebeT QU\u1ea2 NGHI\u00caN C\u1ee8U\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n1. M\u1ee4C TI\u00caU NGHI\u00caN C\u1ee8U\")\n",
    "print(\"-\" * 80)\n",
    "print(\"   X\u00e2y d\u1ef1ng h\u1ec7 th\u1ed1ng ML \u0111\u1ec3 \u0111\u1ec1 xu\u1ea5t chi\u1ebfn thu\u1eadt b\u00f3ng \u0111\u00e1 d\u1ef1a tr\u00ean\")\n",
    "print(\"   c\u00e1c ch\u1ec9 s\u1ed1 tr\u1eadn \u0111\u1ea5u v\u1edbi \u0111\u1ed9 ch\u00ednh x\u00e1c \u226555.56%\")\n",
    "\n",
    "print(\"\\n2. D\u1eee LI\u1ec6U\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"   \u2022 T\u1ed5ng s\u1ed1 m\u1eabu: {len(df):,}\")\n",
    "print(f\"   \u2022 S\u1ed1 \u0111\u1eb7c tr\u01b0ng: {len(feature_columns)}\")\n",
    "print(f\"   \u2022 Ph\u00e2n b\u1ed1 l\u1edbp: Win={result_dist['Win']}, Draw={result_dist['Draw']}, Loss={result_dist['Loss']}\")\n",
    "\n",
    "print(\"\\n3. M\u00d4 H\u00ccNH\")\n",
    "print(\"-\" * 80)\n",
    "print(\"   \u2022 Thu\u1eadt to\u00e1n: Random Forest Classifier\")\n",
    "print(\"   \u2022 T\u1ed1i \u01b0u h\u00f3a: GridSearchCV v\u1edbi 5-fold cross-validation\")\n",
    "print(\"   \u2022 Si\u00eau tham s\u1ed1 t\u1ed1t nh\u1ea5t:\")\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"     - {param}: {value}\")\n",
    "\n",
    "print(\"\\n4. K\u1ebeT QU\u1ea2 HI\u1ec6U SU\u1ea4T\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"   \u2022 Baseline Model:\")\n",
    "print(f\"     - Test Accuracy: {accuracy_baseline*100:.2f}%\")\n",
    "print(f\"     - CV Mean: {cv_scores_baseline.mean()*100:.2f}% (\u00b1{cv_scores_baseline.std()*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\n   \u2022 Optimized Model:\")\n",
    "print(f\"     - Test Accuracy: {accuracy_optimized*100:.2f}%\")\n",
    "print(f\"     - CV Mean: {cv_scores_optimized.mean()*100:.2f}% (\u00b1{cv_scores_optimized.std()*100:.2f}%)\")\n",
    "print(f\"     - C\u1ea3i thi\u1ec7n: +{(accuracy_optimized - accuracy_baseline)*100:.2f}%\")\n",
    "\n",
    "print(f\"\\n   \u2022 \u0110\u1ea1t m\u1ee5c ti\u00eau: {'\u2713 C\u00d3' if accuracy_optimized >= 0.5556 else '\u2717 KH\u00d4NG'}\")\n",
    "print(f\"     ({accuracy_optimized*100:.2f}% {'\u2265' if accuracy_optimized >= 0.5556 else '<'} 55.56%)\")\n",
    "\n",
    "print(\"\\n5. PH\u00c1T HI\u1ec6N QUAN TR\u1eccNG\")\n",
    "print(\"-\" * 80)\n",
    "print(\"   Top 3 ch\u1ec9 s\u1ed1 quan tr\u1ecdng nh\u1ea5t:\")\n",
    "for i, (feat, imp) in enumerate(feature_importance.head(3).values, 1):\n",
    "    print(f\"     {i}. {feat.replace('_', ' ').title()}: {imp:.4f}\")\n",
    "\n",
    "print(\"\\n6. \u1ee8NG D\u1ee4NG TH\u1ef0C T\u1ebe\")\n",
    "print(\"-\" * 80)\n",
    "print(\"   \u2022 H\u1ec7 th\u1ed1ng c\u00f3 th\u1ec3 d\u1ef1 \u0111o\u00e1n k\u1ebft qu\u1ea3 tr\u1eadn \u0111\u1ea5u\")\n",
    "print(\"   \u2022 \u0110\u1ec1 xu\u1ea5t \u0111i\u1ec1u ch\u1ec9nh chi\u1ebfn thu\u1eadt d\u1ef1a tr\u00ean \u0111i\u1ec3m y\u1ebfu\")\n",
    "print(\"   \u2022 H\u1ed7 tr\u1ee3 HLV \u0111\u01b0a ra quy\u1ebft \u0111\u1ecbnh chi\u1ebfn thu\u1eadt\")\n",
    "\n",
    "print(\"\\n7. H\u1ea0NG CH\u1ebe V\u00c0 H\u01af\u1edaNG PH\u00c1T TRI\u1ec2N\")\n",
    "print(\"-\" * 80)\n",
    "print(\"   H\u1ea1n ch\u1ebf:\")\n",
    "print(\"   \u2022 Kh\u00f4ng x\u00e9t \u0111\u1ebfn y\u1ebfu t\u1ed1 t\u00e2m l\u00fd, ch\u1ea5n th\u01b0\u01a1ng\")\n",
    "print(\"   \u2022 D\u1eef li\u1ec7u gi\u1edbi h\u1ea1n \u1edf m\u1ed9t s\u1ed1 gi\u1ea3i \u0111\u1ea5u\")\n",
    "print(\"\\n   H\u01b0\u1edbng ph\u00e1t tri\u1ec3n:\")\n",
    "print(\"   \u2022 Th\u00eam d\u1eef li\u1ec7u real-time tracking\")\n",
    "print(\"   \u2022 K\u1ebft h\u1ee3p deep learning cho video analysis\")\n",
    "print(\"   \u2022 T\u00edch h\u1ee3p y\u1ebfu t\u1ed1 th\u1eddi ti\u1ebft, s\u00e2n nh\u00e0/s\u00e2n kh\u00e1ch\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"K\u1ebeT LU\u1eacN\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Nghi\u00ean c\u1ee9u \u0111\u00e3 th\u00e0nh c\u00f4ng x\u00e2y d\u1ef1ng h\u1ec7 th\u1ed1ng Random Forest \u0111\u1ea1t\")\n",
    "print(f\"\u0111\u1ed9 ch\u00ednh x\u00e1c {accuracy_optimized*100:.2f}%, v\u01b0\u1ee3t m\u1ee5c ti\u00eau 55.56%.\")\n",
    "print(\"H\u1ec7 th\u1ed1ng c\u00f3 th\u1ec3 \u1ee9ng d\u1ee5ng th\u1ef1c t\u1ebf \u0111\u1ec3 h\u1ed7 tr\u1ee3 ph\u00e2n t\u00edch v\u00e0 \u0111\u1ec1 xu\u1ea5t\")\n",
    "print(\"chi\u1ebfn thu\u1eadt trong b\u00f3ng \u0111\u00e1 chuy\u00ean nghi\u1ec7p.\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. L\u01b0u M\u00f4 H\u00ecnh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# L\u01b0u m\u00f4 h\u00ecnh\n",
    "model_filename = 'tactical_rf_model.pkl'\n",
    "scaler_filename = 'tactical_scaler.pkl'\n",
    "\n",
    "with open(model_filename, 'wb') as f:\n",
    "    pickle.dump(rf_optimized, f)\n",
    "\n",
    "with open(scaler_filename, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(\"\u2713 \u0110\u00e3 l\u01b0u m\u00f4 h\u00ecnh v\u00e0 scaler!\")\n",
    "print(f\"  \u2022 Model: {model_filename}\")\n",
    "print(f\"  \u2022 Scaler: {scaler_filename}\")\n",
    "\n",
    "# H\u01b0\u1edbng d\u1eabn s\u1eed d\u1ee5ng\n",
    "print(\"\\n\u0110\u1ec3 s\u1eed d\u1ee5ng m\u00f4 h\u00ecnh \u0111\u00e3 l\u01b0u:\")\n",
    "print(\"\"\"```python\n",
    "import pickle\n",
    "\n",
    "# Load model\n",
    "with open('tactical_rf_model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "# Load scaler\n",
    "with open('tactical_scaler.pkl', 'rb') as f:\n",
    "    scaler = pickle.load(f)\n",
    "\n",
    "# D\u1ef1 \u0111o\u00e1n\n",
    "new_data = scaler.transform([your_match_stats])\n",
    "prediction = model.predict(new_data)\n",
    "```\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T\u00e0i Li\u1ec7u Tham Kh\u1ea3o\n",
    "\n",
    "1. Breiman, L. (2001). Random Forests. Machine Learning, 45(1), 5-32.\n",
    "2. Pedregosa, F., et al. (2011). Scikit-learn: Machine Learning in Python. JMLR 12, pp. 2825-2830.\n",
    "3. Decroos, T., et al. (2019). Actions Speak Louder than Goals: Valuing Player Actions in Soccer. KDD 2019.\n",
    "4. StatsBomb. (2023). Open Data. https://github.com/statsbomb/open-data\n",
    "\n",
    "---\n",
    "\n",
    "**T\u00e1c gi\u1ea3:** Nghi\u00ean c\u1ee9u khoa h\u1ecdc v\u1ec1 ph\u00e2n t\u00edch b\u00f3ng \u0111\u00e1  \n",
    "**Ng\u00e0y ho\u00e0n th\u00e0nh:** 2026  \n",
    "**C\u00f4ng c\u1ee5:** Python 3.x, scikit-learn, pandas, matplotlib, seaborn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}